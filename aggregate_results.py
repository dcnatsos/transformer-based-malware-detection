import os
import re
import statistics
from scipy.stats import ttest_rel, wilcoxon, kstest

def parse_metrics(log_content):
    experiment = {
    'transformer': {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'training_time': [],
    },
    'lstm':
    {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'training_time': [],
    }
}

    # Regex to find lines that start with "Accuracy: [" and capture the values inside the brackets
    patternAcc = re.compile(r"Accuracy: \[([0-9.,\s]+)\]")
    patternPrec = re.compile(r"Precision: \[([0-9.,\s]+)\]")
    patternRec = re.compile(r"Recall: \[([0-9.,\s]+)\]")
    patternF1 = re.compile(r"F1-score: \[([0-9.,\s]+)\]")
    patternATT = re.compile(r"Training Time \(s\): \[([0-9.,\s]+)\]")

    patternsDict = {'accuracy': patternAcc, 'precision': patternPrec, 'recall': patternRec, 'f1': patternF1, 'training_time': patternATT}
    matchesDict = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'training_time': []}

    # Find all matches in the log file content
    for key, pattern in patternsDict.items():
        matches = pattern.findall(log_content)
        matchesDict[key] = matches


    # Save the matching lines to the "Accuracy" list
    for key, matches in matchesDict.items():
        i = 0
        for match in matches:
            i += 1
            # print('\n')
            # print(f"For key: {key}, match {i}:", match)
            values = [float(val.strip()) for val in match.split(',')]
            # print(f"For key: {key}, values {i}:", values)
            if i == 1:
                experiment['transformer'][key] = values
            elif i == 2:
                experiment['lstm'][key] = values
            else:
                raise Exception(f'Found {i} matches instead of 2')

    return experiment

def read_log_file(filepath):
    with open(filepath, 'r') as file:
        content = file.readlines()
    # Concatenate the list of lines into a single string
    log_content = ''.join(content)
    return parse_metrics(log_content)

def load_experiment_logs(log_dir) -> dict[str, dict[str, dict[str, list]]]:
    experiments = {}
    experiment_files = [f'experiment{i}.logs' for i in range(1, 11)]
    for filename in experiment_files:#os.listdir(log_dir):
        if filename.startswith("experiment") and filename.endswith(".logs"):
            filepath = os.path.join(log_dir, filename)
            print(f"Parsing file: {filepath}")
            experiment = read_log_file(filepath)
            print(f"Extracted experiment: {experiment}\n")
            experiments[os.path.splitext(filename)[0]] = experiment
    return experiments

def printSingleExperimentResults(experiments, i = 1):
    experiment = experiments[f'experiment{i}']

    for model, metrics in experiment.items():
        print(f'{model.upper()} metrics:')
        for metric, value in metrics.items():
            print(f'{metric.capitalize()}:', value)
        print('\n')

    for model in experiment.keys():
        print(f'{model.upper()} average metrics:')
        accuracy = experiment[model]['accuracy']
        precision = experiment[model]['precision']
        recall = experiment[model]['recall']
        f1 = experiment[model]['f1']
        training_time = experiment[model]['training_time']

        print(f'Accuracy: Mean = {statistics.mean(accuracy)} Median = {statistics.median(accuracy)} Std = {statistics.stdev(accuracy)}')
        print(f'Precision: Mean = {statistics.mean(precision)} Median = {statistics.median(precision)} Std = {statistics.stdev(precision)}')
        print(f'Recall: Mean = {statistics.mean(recall)} Median = {statistics.median(recall)} Std = {statistics.stdev(recall)}')
        print(f'F1: Mean = {statistics.mean(f1)} Median = {statistics.median(f1)} Std = {statistics.stdev(f1)}')
        print(f'Training Time: Mean = {statistics.mean(training_time)} Median = {statistics.median(training_time)} Std = {statistics.stdev(training_time)}')
        print('\n')


############## MAIN


log_dir = "experiment_series_1_100/logs"
experiments = load_experiment_logs(log_dir)
FOLDS = 10

print('Total experiments:', len(experiments))
# printSingleExperimentResults(experiments, 8)

aggregated_metrics = {
    'transformer': {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'training_time': [],
    },
    'lstm':
    {
        'accuracy': [],
        'precision': [],
        'recall': [],
        'f1': [],
        'training_time': [],
    }
}

for experimentId, experiment in experiments.items():
    for model, metrics in experiment.items():
        for metric, metricValue in metrics.items():
            aggregated_metrics[model][metric].extend(metricValue)

print('Total number of samples per metric per model (i.e. the number of times the model was trained):', len(aggregated_metrics['transformer']['accuracy']))

for model, metrics in aggregated_metrics.items():
    print(f'\n{model.upper()} aggregated metrics:')
    for metric, metricValue in metrics.items():
        print(f'Aggregated {metric.capitalize()}: Mean = {statistics.mean(metricValue)} Median = {statistics.median(metricValue)} Std = {statistics.stdev(metricValue)}')

accuracy_ttest = ttest_rel(aggregated_metrics['transformer']['accuracy'], aggregated_metrics['lstm']['accuracy'])
precision_ttest = ttest_rel(aggregated_metrics['transformer']['precision'], aggregated_metrics['lstm']['precision'])
recall_ttest = ttest_rel(aggregated_metrics['transformer']['recall'], aggregated_metrics['lstm']['recall'])
f1_ttest = ttest_rel(aggregated_metrics['transformer']['f1'], aggregated_metrics['lstm']['f1'])
training_time_ttest = ttest_rel(aggregated_metrics['transformer']['training_time'], aggregated_metrics['lstm']['training_time'])

accuracy_wilcoxon = wilcoxon(aggregated_metrics['transformer']['accuracy'], aggregated_metrics['lstm']['accuracy'])
precision_wilcoxon = wilcoxon(aggregated_metrics['transformer']['precision'], aggregated_metrics['lstm']['precision'])
recall_wilcoxon = wilcoxon(aggregated_metrics['transformer']['recall'], aggregated_metrics['lstm']['recall'])
f1_wilcoxon = wilcoxon(aggregated_metrics['transformer']['f1'], aggregated_metrics['lstm']['f1'])
training_time_wilcoxon = wilcoxon(aggregated_metrics['transformer']['training_time'], aggregated_metrics['lstm']['training_time'])

print(f"\nAccuracy t-test: p-value={accuracy_ttest.pvalue}")
print(f"Precision t-test: p-value={precision_ttest.pvalue}")
print(f"Recall t-test: p-value={recall_ttest.pvalue}")
print(f"F1-score t-test: p-value={f1_ttest.pvalue}")
print(f"Training time t-test: p-value={training_time_ttest.pvalue}")

print(f"\nAccuracy Wilcoxon test: p-value={accuracy_wilcoxon.pvalue}")
print(f"Precision Wilcoxon test: p-value={precision_wilcoxon.pvalue}")
print(f"Recall Wilcoxon test: p-value={recall_wilcoxon.pvalue}")
print(f"F1-score Wilcoxon test: p-value={f1_wilcoxon.pvalue}")
print(f"Training time Wilcoxon test: p-value={training_time_wilcoxon.pvalue}")
