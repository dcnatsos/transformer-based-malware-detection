import tensorflow as tf
import numpy as np

import sys

python3_path = sys.executable
print("Python 3 interpreter being used:", python3_path)

import os
import pandas as pd
import math
import numpy as np
import tensorflow as tf
import shap

# from constants import EPSILON
from preprocessing.preprocessing_simple import preprocessing, tuple_dataset_to_X_y

MODEL_NAME = 'model_for_shap_full.keras'
FIGURE_NAME = 'feature_importance_ig_full_model_all_xtest_probabilities.png'
BATCH_SIZE = 100  # The batch size of the integrated gradients (how many x_test samples to look at a time)

# Define Training, Validation, Test sizes
SAMPLE_NO = 104
TRAIN_SIZE = math.floor(SAMPLE_NO * 0.6)
VALIDATION_SIZE = math.ceil(SAMPLE_NO * 0.2)
TEST_SIZE = math.ceil(SAMPLE_NO * 0.2)

if TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE != SAMPLE_NO:
    raise Exception(f"Bad dataset size:\nSample Number: {SAMPLE_NO}\nTrain Size: {TRAIN_SIZE}\nValidation Size: {VALIDATION_SIZE}\nTest Size: {TEST_SIZE}")

preprocessed_folder = os.path.join(os.getcwd(), '../preprocessed')
datasets, _, feature_names = preprocessing(preprocessed_folder, SAMPLE_NO, TEST_SIZE, VALIDATION_SIZE)

x_train, y_train = tuple_dataset_to_X_y(datasets['train'])
x_val, y_val = tuple_dataset_to_X_y(datasets['val'])
x_test, y_test = tuple_dataset_to_X_y(datasets['test'])

del datasets  # Clear datasets to free memory

print('Data shapes:')
print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)
print(x_test.shape)
print(y_test.shape)

class IntegratedGradients:
    def __init__(self, model):
        self.model = model

    def interpolate_inputs(self, baseline, inputs, alphas):
        """Interpolate between baseline and inputs using alphas."""
        alphas = alphas[:, np.newaxis, np.newaxis]  # Ensure proper shape for interpolation
        return baseline + alphas * (inputs - baseline)

    def compute_gradients(self, inputs):
        """Compute gradients of the output probabilities with respect to the inputs."""
        with tf.GradientTape() as tape:
            tape.watch(inputs)
            # Forward pass through the model
            outputs = self.model(inputs)
            # Here the entire probability vector for the output is used as the target
            target_probs = outputs  # Take the full output probability distribution

        # Compute the gradients of the output probabilities w.r.t. the inputs
        return tape.gradient(target_probs, inputs)  # Calculate gradients for the full probability distribution

    def integrated_gradients(self, inputs, baseline=None, m_steps=50):
        """Compute integrated gradients."""
        if baseline is None:
            baseline = tf.zeros_like(inputs)  # Use zero baseline if none is provided

        # Generate alphas
        alphas = np.linspace(0, 1, m_steps)

        # Initialize integrated gradients to zero
        integrated_grads = tf.zeros_like(inputs)

        # Iterate over interpolation steps
        for alpha in alphas:
            # Interpolate inputs between the baseline and the original inputs
            interpolated_input = self.interpolate_inputs(baseline, inputs, np.array([alpha]))

            # Compute gradients for the interpolated inputs
            grads = self.compute_gradients(interpolated_input)

            # Accumulate gradients over all steps
            integrated_grads += grads / m_steps

        # Scale the integrated gradients by the difference between inputs and baseline
        return (inputs - baseline) * integrated_grads

# Load the model and data
model = tf.keras.models.load_model(MODEL_NAME)

# Initialize IntegratedGradients explainer
ig = IntegratedGradients(model)

# Get the number of batches
num_samples = len(x_test)
num_batches = math.ceil(num_samples / BATCH_SIZE)

# Initialize an empty list to accumulate gradients across batches
all_integrated_grads = []

# Iterate through x_test in batches
for batch_idx in range(num_batches):
    start_idx = batch_idx * BATCH_SIZE
    end_idx = min(start_idx + BATCH_SIZE, num_samples)  # Ensure it doesn't go out of bounds
    
    x_batch = x_test[start_idx:end_idx]
    
    # Compute integrated gradients for each sample in the batch
    batch_integrated_grads = []
    for i in range(len(x_batch)):
        # Compute integrated gradients for the full probability output
        grads = ig.integrated_gradients(tf.convert_to_tensor(x_batch[i:i+1], dtype=tf.float32))
        grads = tf.squeeze(grads, axis=0)  # Remove the extra batch dimension
        batch_integrated_grads.append(grads)
    
    # Append the integrated gradients of this batch to the accumulated list
    all_integrated_grads.extend(batch_integrated_grads)

# Convert the list of tensors to a single tensor or NumPy array
all_integrated_grads = tf.convert_to_tensor(all_integrated_grads)  # Now it should have shape (total_samples, timesteps, features)
integrated_grads_np = all_integrated_grads.numpy()  # Convert the tensor to a NumPy array

print("Integrated Gradients for the test samples:")
print(integrated_grads_np)

# Assuming integrated_grads_np is the output of the IntegratedGradients method
# integrated_grads_np has shape (total_samples, timesteps, features)

# Step 1: Aggregate the attribution values across time steps for each feature
# Take the mean of the absolute values of the attributions across time steps for each feature
feature_importances = np.mean(np.abs(integrated_grads_np), axis=(0, 1))  # Shape: (features,)

# Step 2: Proceed as usual
mean_feature_importances = feature_importances  # Since you already averaged across samples and time steps

# Step 3: Rank the features by their importance
feature_ranking = np.argsort(mean_feature_importances)[::-1]  # Indices of features sorted by importance

# Step 4: Display the most important features
print("Feature ranking (from most important to least important):")
for rank, idx in enumerate(feature_ranking):
    print(f"Rank {rank+1}: Feature {idx} ({feature_names[idx]}) with importance score {mean_feature_importances[idx]}")

# Plot the feature importances

import matplotlib.pyplot as plt

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.bar(range(len(mean_feature_importances)), mean_feature_importances[feature_ranking], tick_label=[feature_names[idx] for idx in feature_ranking])
plt.xlabel('Feature')
plt.ylabel('Importance Score')
plt.title('Feature Importance based on Integrated Gradients')
plt.xticks(rotation=90)  # Rotate the x labels if they overlap

# Save the figure locally
plt.tight_layout()
plt.savefig(FIGURE_NAME)

# Close the plot to free memory
plt.close()

