import os
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Step 1: Read all CSV files into pandas dataframes
data_path = os.path.join(os. getcwd(), 'cloud_malware_2019_small', 'malware_csv')
csv_files = os.listdir(data_path)
dfs = [pd.read_csv(os.path.join(data_path, file)) for file in csv_files] # A list of dfs, 1 csv = 1 df

if len(csv_files) != len(dfs): # TODO: Delete this debug
    raise Exception("Something went really bad")

experiments_ids = []

# Add a new column with the name of the CSV file
for i, df in enumerate(dfs): # returns (index, df_object)
    df['experiment_id'] = csv_files[i]
    experiments_ids.append(csv_files[i])


combined_df = pd.concat(dfs) # Should return a df #SubsamplesNumber (rows) x 45 (columns)

# Step 2: Scaling and one-hot encoding the data
numerical_cols = ['cpu_percent', 'cpu_num', 'cpu_user', 'cpu_sys', 'cpu_children_sys',
    'cpu_children_user', 'num_threads', 'mem_shared', 'mem_data',
    'mem_vms', 'mem_rss', 'mem_dirty', 'mem_swap', 'mem_lib', 'mem_uss', 'mem_text', 
    'io_write_bytes','io_write_chars', 'io_write_count', 'io_read_bytes','io_read_chars', 
    'io_read_count', 'kb_sent', 'kb_received', 'ionice_ioclass', 'ionice_value', 'nice', 
    'ctx_switches_voluntary', 'ctx_switches_involuntary', 'gid_effective', 'num_fds',
]
categorical_cols = ['name', 'status']
exclude_cols = ['sample_no', 'exp_no', 'vm_id', 'pid', 'ppid', 'sample_time', 
    'process_creation_time', 'gid_real', 'gid_saved', 'mem_pss', 'label', 'experiment_id'
]

################## Debug ##################
existing_cols = [True if element in combined_df.columns else False for element in numerical_cols]
not_existing_cols = [element for element in df.columns if element not in numerical_cols + categorical_cols + exclude_cols]
not_existing_cols_ = [element for element in numerical_cols + categorical_cols + exclude_cols if element not in df.columns]

if not_existing_cols != not_existing_cols_:
    raise Exception('Something went bad 2')
################## Debug ##################

# TODO: Maybe prefer hashing encoding or frequency encoding for categorical values, 
# because one-hot encoding significantly increases the feature space vector, by 101 new features (initially 45), which is 200% increase
# TODO: Do a PCA to see if the 'name' attribute increases anyhow the performance of the model
# Scalce and one-hot encode per experiment or the whole dataset?
# Define transformers for numerical and categorical columns -> The logic says per dataset
# Well, if we encode the categorical with frequency encoding, then services with a high occuring name are more likely,
# to be labeled as non-malicious, however the problem lies when a malware changes its name to a common name process, then
# we might have a lot of false negatives.


numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values if any
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values if any
    ('onehot', OneHotEncoder(sparse_output=False))
])

# Combine transformers using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ],
    remainder='passthrough'  # Include other columns without transformation
)

# Fit and transform the data
transformed_data = preprocessor.fit_transform(combined_df)

# Get feature names after one-hot encoding
feature_names = numerical_cols + list(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)) + exclude_cols

# Convert transformed data back to DataFrame
transformed_df = pd.DataFrame(transformed_data, columns=feature_names)

# Rearrange the columns (bring the informative, excluded columns at the begining of the dataset)
transformed_df = transformed_df[transformed_df.columns[-12:].tolist() + transformed_df.columns[:-12].tolist()]

# transformed_df.to_csv('output_all.csv', index=False)

experiment_dfs = {}

for experiment_id in transformed_df['experiment_id'].unique():
    experiment_dfs[experiment_id] = transformed_df[transformed_df['experiment_id'] == experiment_id].copy()

# Write each dataframe to a CSV file with experiment_id as filename under the folder preprocess/
output_folder = "preprocessed"

# Create the output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

# Export the preprocesses csvs of each experiment
for experiment_id, df in experiment_dfs.items():
    output_filename = os.path.join(output_folder, experiment_id)
    df.to_csv(output_filename, index=False)
    print(f"DataFrame with experiment_id {experiment_id} saved to {output_filename}")



# print(len(numerical_cols))
# existing_cols[0] =

# if existing_cols:
#     print('ok')
# print(len(numerical_cols))
# print()

# print(combined_df.columns)
# print(numerical_cols)
# print(categorical_cols)
# exit()

# Visualizing data
# print(combined_df.columns)

# print(combined_df['sample_time'].size)

# Playing wit IDs and combined df
# print(combined_df['experiment_id'].unique())
# temp = combined_df['experiment_id'].unique()
# print(experiments_ids[0])
# print(combined_df[combined_df['experiment_id'] == experiments_ids[0]])

# Step 2: Scale and one hot encode the data (keep only the necessary columns presented within the paper)
#  They do not use mem_uss, gid_saved and gid_real


# Step 3: Create the sequences
# def find_samples(df, csv_name):
#     samples = []
#     sample = []
#     current_time = None
    
#     for index, row in df.iterrows():
#         # Assuming 'sample_time' is the name of the column containing timestamps
#         timestamp = row['sample_time']
        
#         if current_time is None:
#             current_time = timestamp
#             sample.append(row)
#         elif timestamp == current_time:
#             sample.append(row)
#         else:
#             sample_df = pd.concat(sample)  # Concatenate rows to form a sample
#             sample_df['csv_name'] = csv_name  # Add column with CSV name
#             samples.append(sample_df)
#             sample = [row]
#             current_time = timestamp
    
#     if sample:  # Append the last sample
#         sample_df = pd.concat(sample)
#         sample_df['csv_name'] = csv_name
#         samples.append(sample_df)
    
#     return samples

# sampled_data = []
# for df, csv_name in zip(dfs, csv_files): # TODO: Check if each dfs corresponds to the correct csv
#     samples = find_samples(df, csv_name)
#     sampled_data.extend(samples)

# Now, sampled_data contains a list of pandas DataFrames, where each DataFrame represents a sample
# and contains an additional column 'csv_name' indicating the CSV (experiment) it came from.
    
# Step 4: Split into train, validation and test
    
# Step 5: Train and tune the model
    
# Step 6: Test the model

# print(sampled_data)
# print(type(sampled_data))
# print(len(sampled_data))
# print(sampled_data[0])
# print(type(sampled_data[0]))
