import os
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


df = pd.read_csv('output_all.csv')

print(df.shape)
print(df.columns)

# Step 1: Read all CSV files into pandas dataframes
# data_path = os.path.join(os. getcwd(), 'cloud_malware_2019_small', 'malware_csv')
# csv_files = os.listdir(data_path)
# dfs = [pd.read_csv(os.path.join(data_path, file)) for file in csv_files]

# dfs = pd.read_csv(os.path.join(data_path, csv_files[0]))


# print(dfs.loc[dfs['pid'] > 1015])

# # print(dfs)
# # print(dfs.columns)
# # print(dfs.loc[dfs['label'] != 18].size)
# seq_timestamps = dfs['sample_time'].unique()

# print(seq_timestamps)
# print(len(seq_timestamps))


# df_filtered = df[df['pid'] == 5]

# 1) Merge all csv data into one dataframe, keeping their experiment number
# 2) Scale the data and preprocess them
# 3) Create the sequences (per timestamp per experiment) or you can split them back based on the experiment to make sure of the uniquenece of the timestamp
# 4) Split into train, validation and test (60%, 20%, 20%)
