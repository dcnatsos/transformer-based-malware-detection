{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7045ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3 interpreter being used: /home/issel/Workspace/transformer-based-malware-detection/venv_gpu/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "python3_path = sys.executable\n",
    "print(\"Python 3 interpreter being used:\", python3_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0c1173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sample_time', 'label', 'experiment_id', 'cpu_percent', 'cpu_num',\n",
      "       'cpu_user', 'cpu_sys', 'cpu_children_sys', 'cpu_children_user',\n",
      "       'num_threads', 'mem_shared', 'mem_data', 'mem_vms', 'mem_rss',\n",
      "       'mem_dirty', 'mem_swap', 'mem_lib', 'mem_uss', 'mem_text',\n",
      "       'io_write_bytes', 'io_write_chars', 'io_write_count', 'io_read_bytes',\n",
      "       'io_read_chars', 'io_read_count', 'kb_sent', 'kb_received',\n",
      "       'ionice_ioclass', 'ionice_value', 'nice', 'ctx_switches_voluntary',\n",
      "       'ctx_switches_involuntary', 'gid_effective', 'num_fds',\n",
      "       'status_disk-sleep', 'status_running', 'status_sleeping'],\n",
      "      dtype='object')\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Define Training, Validation, Test sizes\n",
    "SAMPLE_NO = 104 #8\n",
    "TRAIN_SIZE = math.floor(SAMPLE_NO*0.6)\n",
    "VALIDATION_SIZE = math.ceil(SAMPLE_NO*0.2)\n",
    "TEST_SIZE = math.ceil(SAMPLE_NO*0.2)\n",
    "\n",
    "if TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE != SAMPLE_NO:\n",
    "    raise Exception(f\"Bad dataset size:\\nSample Number: {SAMPLE_NO}\\nTrain Size: {TRAIN_SIZE}\\nValidation Size: {VALIDATION_SIZE}\\nTest Size: {TEST_SIZE}\")\n",
    "\n",
    "# Read each preprocessed CSV file representing an experiment\n",
    "data_path = os.path.join(os.getcwd(), 'preprocessed')\n",
    "csv_files = os.listdir(data_path)[:SAMPLE_NO] # Use only the first SAMPLE_NO (for testing the training.py)\n",
    "dfs = [pd.read_csv(os.path.join(data_path, file)) for file in csv_files] # A list of dfs, 1 csv = 1 df\n",
    "\n",
    "# Drop specified columns from all dataframes\n",
    "columns_to_drop = [\n",
    "'sample_no', 'exp_no', 'vm_id', 'pid', 'ppid', 'process_creation_time', 'gid_real',\n",
    "'gid_saved', 'mem_pss', #'experiment_id' , 'sample_time', 'label' # These three are dropped later\n",
    "] \n",
    "\n",
    "# Drop unecessary columns\n",
    "for df in dfs:\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    df.drop(columns=[s for s in df.columns if s.startswith(\"name_\")], inplace=True) # DROP THE 'name' feature\n",
    "\n",
    "print(dfs[0].columns)\n",
    "print(len(dfs[0].columns))\n",
    "\n",
    "# Split labels into a different df?\n",
    "\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7649c2ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['a213ebd69fbc11d612d0374b373f65d8.csv', '6810865879e18f3db71416367cebf5b4758b15739cae22ca129ed8787f09283d.csv', '1e5eb137f762df0b8b291ba3c682abcc.csv', '224ce02b25ef000afa24859850427e1a.csv', '5a16c12e1abe11317465ea4032aa25aa.csv', 'fee439d5d49f6a4093a82216ef33e66f.csv', 'b1338cd9b5a853d8920f5a868108135b.csv', '9579470ab9f50bc231d0d8f1f6065c19.csv', 'dd8547fd7060239347a922474b1323ca.csv', 'cf18efd364e0224542f2ebb9c7432508.csv', '58c23ca549c941f0d44b35fa31d77011.csv', 'e3768c5eb6d82a0c5fc00c2dc779135f.csv', '6ab93eb58bec3075278a44825ea0d0e3.csv', 'ac8325d04a8940150f05e00fefb8a9df.csv', 'fe7effc7ea7361e291d935ba93beeb05.csv', '25e15bb7f7d46083d93c3193f91f5d31.csv', '35ad8f5e188d8c4bf093900ab655aefe.csv', '5dc3c90df617d1789fa96308abbf82ca.csv', 'c2576aeff0fd9267b6cc3a7e1089e05d.csv', '1a5cf9b588d275b1914746c7a948eb44.csv', '5da5ec095607301a0c34f580a2e4b392.csv', 'fa1d1861e29d664b8bc381640ad40e9f.csv', '5b1176a690feaa128bc83ad278b19ba8.csv', '27989622568e38e7a314fc2293177fae496f9795e91321f7eb26e09e2a0e974b.csv', '9661ee2c5691893965d298d5e44d5372.csv', '441ab8542f9aab3cee0b9e0db35a3ed8.csv', '9229ffbea49d8044e7391fcfae627165.csv', '66f865d6eadbbf0c4111096428ea50cb.csv', '31f3a8e6bc25566d70ddd06092339a36.csv', '1237582ed31e286b429c679e7b4295f7.csv', '4ffa98be139a46a8df51c887e2a01e89.csv', '6537a127bb1d9101115628f9e5d76e35.csv', 'd1d9ea890a4357bfd9abd5bda588fbb8.csv', '6cdee514ffec725bbf007b145cdc745a.csv', 'befeff899d9934402a728399f52094cd.csv', 'de347cda7f29afa53509f9966b631cb5.csv', 'a583399e3e632a204b2d0297a642343b.csv', '288e03c5619a51dd93381cbbca7da477.csv', 'c7d7e861826a4fa7db2b92b27c36e5e2.csv', '874670013feca4e9a3b2f3d51b9887fb1909e1e89387e6542727f1b09a2d26eb.csv', '7498314786dddf730a306c454b65dca907bfe7ae2ba6d35463577c8d9608e7dc.csv', 'd45a89ada82a5ae80d2c01d01a1cb256.csv', '8895114572dee308d2c73af1cd81bccf6968602310e513be460174af89c8a929.csv', '324cc7e561bc231e0e4f304be420b12a.csv', '443441514a79c35b6fc9cc074f68694bc152c519bb0dc6c318db316724c9ab70.csv', '1692f24d5b140d8e3ccf7716c10abc43.csv', '896169484c8aea1fd10ec8fa62fdb8da41f053c7aebc3d05e9f642e9c4614858.csv', '04ab137aca2e4ec0981c2bac34ed6126.csv', '2bfbfc0d4295c6747a452fbe2a9cfcbf.csv', 'd43d7bd122e0abcd79da10873f201aea.csv', '238459a71c67d52920c0fc36574f7852.csv', '94239e445dc6501310fce77509c14570.csv', '7288222f5481e761e9c5ce3680861e84.csv', '5d10bcb15bedb4b94092c4c2e4d245b6.csv', '4465366066353bbed850273b9ee201484b7cc027536b11c37a244e7262d46f9b.csv', '7539917767b08c5949a61aedcd04d6b1.csv', 'ec325d136f73e36eeb0a762dbd6dead8.csv', 'ab62ea3634bb269f2a734932cb5daf9b.csv', 'a2fe69ca30817337c7c22c266187194a.csv', '3c0e73b3e9f4ecda3941bc873478afdf.csv', 'f768c52ab90d869664a1640b1b630631.csv', '10f5beac257a92665866cdc99550b7bb.csv']\n",
      "Validation ['bd4ca9ea8269face1f68ddfb6b2f6d78.csv', '63a1aee00988160a2024f492fa29e8e7.csv', '019c04e16cc6765ec227d5809dcfa4a3.csv', '706215905f84e18be109bc33427ef221.csv', '32b7ea7e6ca69ac754cff9bf15b92009.csv', '4198ae1120dedfd9889b15e1dbe5016d.csv', 'c4c86629ac290ed176716e530b22f121.csv', '595094c92145c10860fff3f85cbe6174.csv', '3fa8dd9c1ff86fbafd5f2bbfb1bf4f97.csv', '14c6ae25888605c15c37e456b7ac53ac.csv', '18f6167f853a8bd59104b7b11d7337ca.csv', 'cd98c2b681ff2517c1fe42cd166c2477.csv', '5d8484f3d0e8275f6b12e933b932f483.csv', '9721076317dcc930acd118cbbd87fd248ee961a348dc427c5360e7fc845c47a3.csv', '3c00db6a98b46fb40e85f1ff910ebfb3.csv', '3f5c73745f7c17702bac0642a85d7d80.csv', '407067164867eacf8fae3493db6e337618d5d13c6c141e8a3572b35d1626b206.csv', '770061775efb653ab9c22d49ed96521c200f8efe9e4a9471a122d95a18856474.csv', '4928367035712e199f957c73bd244d4786fd7c5cd8fbf610689905e209c6b6da.csv', 'e1facb220a3d0e61abc1a255e3e3b93d.csv', 'fa2162361bb7529a221e39968353a5b7.csv']\n",
      "Test: ['94634938825a80a330aa2f120738852384b86f12a2394d381507b4c37745fb70.csv', '195402314716caadc5c60436a31bb34c2018d24d2c404d32dfa2ccfa0ee75797.csv', '6374630490f0d4a2f0eb5660c5a5c6e9229008a187ae4b4bbf5c282bea93360f.csv', 'c2b808571f41176a408ad6a0e3838766.csv', 'ab9398fb7b5f84440c24cd0ef53839cf.csv', '1de8bec080347603adab9cd05e3cf16b.csv', '83f161ae3746cf7b63a8dcb60c659cec.csv', 'b063304f9f18619136dcd2584461bd1b.csv', '93f917c0b6d318fde33cb7bebef0ddea.csv', '1235543381224e7574493d9bfb8150fddd9077d5e77603f1cab5abf510c3e54c.csv', 'b4ba07c4d9b781635b33d485b73a614f.csv', '852636368f72c4621f42ff9ca16f4c645b7ab043ea57679d361f8a8b672030dd.csv', '912980125295e5765a0d73ff315a9e51177c34615897ddbf0356f5e6dd7a00bb.csv', '3788a1f0d003114ecb95f70575edc431.csv', '562137459fb942328149138bcd23195611c891e6fbd85a3514a05af092c5bf6a.csv', 'ded7667e7d32f4fbc5dc0fe8d59b0db0.csv', '3aad30955ed6946670efea0dac9ba9b4.csv', 'e0d4b54d790d14f82e9516860a1998bf.csv', '1787727951a37a14c2feb8799f1604bac51632499df99831ea40ddaf209e29f1.csv', '9746054219bfa20e0bf55a066acd447a8878913d4b857057729a579cb1a078b3.csv', '10f32afd3981d35d196f73aa63293dec.csv']\n",
      "Split: train sample size:  16700\n",
      "Split: val sample size:  5616\n",
      "Split: test sample size:  5897\n",
      "Datasets keys:  dict_keys(['train', 'val', 'test'])\n",
      "<class 'list'>\n",
      "5616\n",
      "<class 'tuple'>\n",
      "2\n",
      "<class 'list'>\n",
      "117\n",
      "<class 'list'>\n",
      "34\n",
      "Hello 96\n",
      "Length X: 16700\n",
      "Hello 117\n",
      "Length X: 5616\n",
      "Hello 118\n",
      "Length X: 5897\n",
      "(16700, 227, 34)\n",
      "(16700, 2)\n",
      "(5616, 227, 34)\n",
      "(5616, 2)\n",
      "(5897, 227, 34)\n",
      "(5897, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data by experiment into training, validation, and testing sets\n",
    "train_experiments, test_experiments = train_test_split(csv_files, test_size=(TEST_SIZE+VALIDATION_SIZE), random_state=42)\n",
    "val_experiments, test_experiments = train_test_split(test_experiments, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "print('Train:', train_experiments)\n",
    "print('Validation', val_experiments)\n",
    "print('Test:', test_experiments)\n",
    "\n",
    "# Collect data for each set and transform into series format\n",
    "datasets = {}\n",
    "for split, experiments in zip([\"train\", \"val\", \"test\"], [train_experiments, val_experiments, test_experiments]): # (('train', [1, 2]), ('val', [3]), ('test', [4]))\n",
    "    \n",
    "    # data contains the concatenated train/validation/test dataframe in the respective iteration (3 iterations)\n",
    "    data = pd.concat([dfs[csv_files.index(exp)] for exp in experiments]) \n",
    "\n",
    "    series_data = []\n",
    "    for experiment_id, df in data.groupby('experiment_id'): # Per each dataframe (train, validation, test) group by experiment id\n",
    "        snapshots = df.groupby('sample_time') # And then group by sample time per each experiment\n",
    "        for timestamp, group in snapshots:\n",
    "\n",
    "            # Check if all labels within the snapshot are the same\n",
    "            if group['label'].nunique() > 1:\n",
    "                raise ValueError(\"Snapshot contains different labels\")\n",
    "            \n",
    "            # Determine the class label for the snapshot\n",
    "            label = 1 if any(group['label']) else 0\n",
    "            \n",
    "            # Append a tuple containing series data and label\n",
    "            series = group.drop(columns=['sample_time', 'experiment_id', 'label']).values.tolist()\n",
    "            series_data.append((series, label))\n",
    "\n",
    "    datasets[split] = series_data\n",
    "\n",
    "    if len(datasets[split]) != pd.concat([dfs[csv_files.index(exp)] for exp in experiments])['sample_time'].unique().size:\n",
    "        raise ValueError(f\"Series sample number are not the same for split: {split}\")\n",
    "\n",
    "    print(f'Split: {split} sample size: ',len(datasets[split]))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    - datasets -> dict: A dictionary containing the data splitted in training, validation, test set\n",
    "    - datasets.train/val/test -> list: A list of tuples, each one containing:\n",
    "    - datasets.train/val/test[0] -> tuple: Tuple i\n",
    "    - datasets.train/val/test[i][0] -> list: snapshot/subsamples list of the i-th tuple/snapshot\n",
    "    - datasets.train/val/test[i][1] -> int: label of the i-th tuple/snapshot\n",
    "    - datasets.train/val/test[i][j][0] -> int: feature vector of the j-th process and the i-th tuple/snapshot\n",
    "\n",
    "    -- Each tuple refers to a specific snapshot on a given timestamp of the system processes. The label of the tuple\n",
    "    states whether the system was infected at that time or not. \n",
    "    -- (the concept of experiment_id is used up until splitting\n",
    "    into train/validation/test where all the snapshots from the same experiment are maintained within one of train/val/test\n",
    "    and not splitted. After that, the experiment_id concept is deprecated)\n",
    "    -- The infectious process is probably the one that has the same name with the csv -> TODO: Implement this later.\n",
    "\"\"\"\n",
    "\n",
    "print('Datasets keys: ', datasets.keys())\n",
    "print(type(datasets['val'])) # Validation List\n",
    "print(len(datasets['val']))\n",
    "print(type(datasets['val'][0])) # A snapshot/sample tuple (subsamples list, label)\n",
    "print(len(datasets['val'][0]))\n",
    "print(type(datasets['val'][0][0])) # Subsamples list\n",
    "print(len(datasets['val'][0][0]))\n",
    "print(type(datasets['val'][0][0][0])) # Feature vector of a specific process at a specific timestamp/snapshot/sample\n",
    "print(len(datasets['val'][0][0][0]))\n",
    "datasets['train'][0][0]\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\"\n",
    "    Prepares data for LSTM training.\n",
    "\n",
    "    Args:\n",
    "      data: A list of tuples containing (series, label).\n",
    "          - series: A list of lists, where each inner list represents features \n",
    "                    of all processes at a specific timestamp within the snapshot.\n",
    "          - label: An integer representing the class label (0 or 1) for the entire snapshot.\n",
    "\n",
    "    Returns:\n",
    "      A tuple containing two numpy arrays:\n",
    "          - X: Input data for the LSTM, shaped (samples, feature_dim).\n",
    "          - y: Labels, shaped (samples,).\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for series, label in data:\n",
    "        # Get the number of features per process (assuming all processes have the same)\n",
    "        feature_dim = len(series[0])\n",
    "        # The entire snapshot is considered as a single sequence\n",
    "        X.append(series)\n",
    "        y.append(label)\n",
    "      # Convert to numpy arrays for efficient processing\n",
    "    print('Length X:',len(X))\n",
    "    print('Length Y:',len(y))\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "train_data, val_data, test_data = datasets['train'], datasets['val'], datasets['test']\n",
    "\n",
    "## The idea if to have x_train_i = [[[f1], [f2], [f3], ... , [fn]]] y_train_i = [[label_i]], f1 = feature vector of process 1 of snapshot i\n",
    "## This is multivariate timeseries\n",
    "\n",
    "## Single-variate (easily scaled to multi-variate): https://www.youtube.com/watch?v=c0k-YLQGKjY&ab_channel=GregHogg\n",
    "## Multivariate (part 2) at: https://www.youtube.com/watch?v=kGdbPnMCdOg&ab_channel=GregHogg (26:00+)\n",
    " \n",
    "# TODO: Use jupyter notebook with vs-code, so that I dont have to train new models every time\n",
    "\n",
    "MAX_PROCESSES_LEN = 227 # The paper says 120 but there are some with 190\n",
    "MAX_FEATURES_LEN = 34 # This is the number of features per process \n",
    "\n",
    "def tuple_dataset_to_X_y(dataset):\n",
    "    print('Hello',len(dataset[0][0]))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(dataset)): # -1 ???\n",
    "        row = dataset[i][0] # the i-th snapshot [[f0] [f1] [f2] ... [fn]] (vector of feature vectors/ aka multivariate timeseries)\n",
    "        \n",
    "        # Padding, 120 is the maximum number of processes as defined by the paper, but 131 is in the real dataset, and the minimum is 1\n",
    "        while len(row) < MAX_PROCESSES_LEN:\n",
    "            row.append(np.zeros(len(row[0])))\n",
    "\n",
    "        X.append(row)\n",
    "        label = dataset[i][1]\n",
    "        y.append(label)\n",
    "\n",
    "    # max_p = -1\n",
    "    # max_p_index = -1\n",
    "    # max_f = -1\n",
    "    # max_f_j_index = -1\n",
    "    # max_f_k_index = -1\n",
    "\n",
    "    # for j in range(len(X)):\n",
    "    #     if (len(X[j]) > max_p):\n",
    "    #         max_p = len(X[j])\n",
    "    #         max_p_index = j\n",
    "    #     for k in range(len(X[j])):\n",
    "    #         if len(X[j][k]) > max_f:\n",
    "    #             max_f = len(X[j][k])\n",
    "    #             max_f_k_index = k\n",
    "    #             max_f_j_index = j\n",
    "\n",
    "    # print(f'The maximum number of processes is {max_p} at index {max_p_index}')\n",
    "    # print(f'The lengthiest number of features is {max_f} at j index {max_f_j_index} and k index {max_f_k_index}')\n",
    "    # raise Exception('Stop here')\n",
    "    \n",
    "    # One hot encode labels\n",
    "    y_np = np.array(y)    \n",
    "    y_encoded = np.eye(2)[y_np]\n",
    "\n",
    "    # TODO: Use this to find larger rows (solves Inhomogenous array error) \n",
    "    length = len(X[0])\n",
    "    print('Length X:', len(X))\n",
    "    for i in range(len(X)):\n",
    "        if len(X[i]) != length:\n",
    "            raise Exception(f'Length X[{i}]: {len(X[i])} is different than length: {length}')\n",
    "\n",
    "    return np.array(X), y_encoded\n",
    "\n",
    "\n",
    "# print(datasets['train'])\n",
    "# print(datasets['val'])\n",
    "# print(datasets['test'])\n",
    "\n",
    "x_train, y_train = tuple_dataset_to_X_y(datasets['train'])\n",
    "x_val, y_val = tuple_dataset_to_X_y(datasets['val'])\n",
    "x_test, y_test = tuple_dataset_to_X_y(datasets['test'])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# l_t = len(x_train[0])\n",
    "# lF_t = len(x_train[0][0])\n",
    "\n",
    "# print(range(len(x_train)))\n",
    "\n",
    "# for i in range(len(x_train)):\n",
    "#     if len(x_train[i]) != l_t:\n",
    "#         raise Exception(f'Wrong length 1: {l_t}')\n",
    "#     else:\n",
    "#         print(f'I: {i} l_t: {l_t}')\n",
    "#     for j in range(len(x_train[i])):\n",
    "#         if len(x_train[i][j]) != lF_t:\n",
    "#             raise Exception(f'Wrong length 2: {lF_t}')\n",
    "#         else:\n",
    "#             print(f'(I, J): {i},{j} lF_t: {lF_t}')\n",
    "\n",
    "\n",
    "# print(type(x_train))\n",
    "# print(len(x_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d342f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 12:35:07.261598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 12:35:07.261629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 12:35:07.262704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-16 12:35:07.268208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-16 12:35:07.716299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-16 12:35:08.339293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.373879: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.376501: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.380156: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.382724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.385126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.544294: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.545468: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.546562: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-16 12:35:08.547604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4514 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 227, 256)          297984    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 227, 256)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 227, 128)          197120    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 227, 128)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 544642 (2.08 MB)\n",
      "Trainable params: 544642 (2.08 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 12:35:11.929273: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-04-16 12:35:12.593281: I external/local_xla/xla/service/service.cc:168] XLA service 0x72df827380a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-16 12:35:12.593309: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5\n",
      "2024-04-16 12:35:12.597206: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713260112.664062  342089 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 19s 31ms/step - loss: 0.6917 - accuracy: 0.5397 - val_loss: 0.6875 - val_accuracy: 0.5597\n",
      "Epoch 2/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.6901 - accuracy: 0.5400 - val_loss: 0.6864 - val_accuracy: 0.5597\n",
      "Epoch 3/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.6430 - accuracy: 0.6019 - val_loss: 0.5659 - val_accuracy: 0.6959\n",
      "Epoch 4/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5676 - accuracy: 0.6859 - val_loss: 0.5638 - val_accuracy: 0.6959\n",
      "Epoch 5/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5674 - accuracy: 0.6860 - val_loss: 0.5632 - val_accuracy: 0.6960\n",
      "Epoch 6/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5667 - accuracy: 0.6859 - val_loss: 0.5631 - val_accuracy: 0.6959\n",
      "Epoch 7/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5668 - accuracy: 0.6862 - val_loss: 0.5626 - val_accuracy: 0.6959\n",
      "Epoch 8/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5655 - accuracy: 0.6863 - val_loss: 0.5613 - val_accuracy: 0.6960\n",
      "Epoch 9/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5662 - accuracy: 0.6884 - val_loss: 0.5631 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5651 - accuracy: 0.6860 - val_loss: 0.5602 - val_accuracy: 0.6960\n",
      "Epoch 11/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5602 - accuracy: 0.6905 - val_loss: 0.5484 - val_accuracy: 0.7069\n",
      "Epoch 12/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.5412 - accuracy: 0.7105 - val_loss: 0.5281 - val_accuracy: 0.7235\n",
      "Epoch 13/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.4903 - accuracy: 0.7447 - val_loss: 0.5117 - val_accuracy: 0.7033\n",
      "Epoch 14/100\n",
      "522/522 [==============================] - 15s 29ms/step - loss: 0.4657 - accuracy: 0.7596 - val_loss: 0.4962 - val_accuracy: 0.7188\n",
      "Epoch 15/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.4499 - accuracy: 0.7729 - val_loss: 0.4618 - val_accuracy: 0.7680\n",
      "Epoch 16/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.4371 - accuracy: 0.7808 - val_loss: 0.4676 - val_accuracy: 0.7742\n",
      "Epoch 17/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.4211 - accuracy: 0.7929 - val_loss: 0.4325 - val_accuracy: 0.7895\n",
      "Epoch 18/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.4089 - accuracy: 0.8013 - val_loss: 0.4376 - val_accuracy: 0.7765\n",
      "Epoch 19/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.3896 - accuracy: 0.8171 - val_loss: 0.4352 - val_accuracy: 0.7714\n",
      "Epoch 20/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.3723 - accuracy: 0.8272 - val_loss: 0.3879 - val_accuracy: 0.8175\n",
      "Epoch 21/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.3531 - accuracy: 0.8381 - val_loss: 0.3744 - val_accuracy: 0.8305\n",
      "Epoch 22/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.3314 - accuracy: 0.8511 - val_loss: 0.3658 - val_accuracy: 0.8323\n",
      "Epoch 23/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.3045 - accuracy: 0.8690 - val_loss: 0.3353 - val_accuracy: 0.8522\n",
      "Epoch 24/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.2869 - accuracy: 0.8796 - val_loss: 0.3133 - val_accuracy: 0.8718\n",
      "Epoch 25/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.2680 - accuracy: 0.8886 - val_loss: 0.2940 - val_accuracy: 0.8837\n",
      "Epoch 26/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.2443 - accuracy: 0.9040 - val_loss: 0.2317 - val_accuracy: 0.9184\n",
      "Epoch 27/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.2281 - accuracy: 0.9114 - val_loss: 0.3291 - val_accuracy: 0.8401\n",
      "Epoch 28/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.2232 - accuracy: 0.9129 - val_loss: 0.2325 - val_accuracy: 0.9097\n",
      "Epoch 29/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.2026 - accuracy: 0.9217 - val_loss: 0.2411 - val_accuracy: 0.9119\n",
      "Epoch 30/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1959 - accuracy: 0.9251 - val_loss: 0.1686 - val_accuracy: 0.9409\n",
      "Epoch 31/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1911 - accuracy: 0.9257 - val_loss: 0.1614 - val_accuracy: 0.9452\n",
      "Epoch 32/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1768 - accuracy: 0.9326 - val_loss: 0.2226 - val_accuracy: 0.9197\n",
      "Epoch 33/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1698 - accuracy: 0.9337 - val_loss: 0.1645 - val_accuracy: 0.9382\n",
      "Epoch 34/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1632 - accuracy: 0.9362 - val_loss: 0.1419 - val_accuracy: 0.9466\n",
      "Epoch 35/100\n",
      "522/522 [==============================] - 15s 30ms/step - loss: 0.1605 - accuracy: 0.9346 - val_loss: 0.1551 - val_accuracy: 0.9350\n",
      "Epoch 36/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1556 - accuracy: 0.9390 - val_loss: 0.1627 - val_accuracy: 0.9396\n",
      "Epoch 37/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1536 - accuracy: 0.9395 - val_loss: 0.1531 - val_accuracy: 0.9387\n",
      "Epoch 38/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1533 - accuracy: 0.9389 - val_loss: 0.1569 - val_accuracy: 0.9402\n",
      "Epoch 39/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1491 - accuracy: 0.9390 - val_loss: 0.1650 - val_accuracy: 0.9364\n",
      "Epoch 40/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1426 - accuracy: 0.9435 - val_loss: 0.1177 - val_accuracy: 0.9585\n",
      "Epoch 41/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1428 - accuracy: 0.9406 - val_loss: 0.1144 - val_accuracy: 0.9541\n",
      "Epoch 42/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1402 - accuracy: 0.9417 - val_loss: 0.1057 - val_accuracy: 0.9580\n",
      "Epoch 43/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1397 - accuracy: 0.9423 - val_loss: 0.2097 - val_accuracy: 0.9193\n",
      "Epoch 44/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1383 - accuracy: 0.9412 - val_loss: 0.1028 - val_accuracy: 0.9583\n",
      "Epoch 45/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1331 - accuracy: 0.9434 - val_loss: 0.1171 - val_accuracy: 0.9598\n",
      "Epoch 46/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1330 - accuracy: 0.9443 - val_loss: 0.1228 - val_accuracy: 0.9535\n",
      "Epoch 47/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1305 - accuracy: 0.9454 - val_loss: 0.1303 - val_accuracy: 0.9489\n",
      "Epoch 48/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1285 - accuracy: 0.9450 - val_loss: 0.1063 - val_accuracy: 0.9630\n",
      "Epoch 49/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1318 - accuracy: 0.9432 - val_loss: 0.1824 - val_accuracy: 0.9236\n",
      "Epoch 50/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1249 - accuracy: 0.9459 - val_loss: 0.1548 - val_accuracy: 0.9279\n",
      "Epoch 51/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1245 - accuracy: 0.9469 - val_loss: 0.0914 - val_accuracy: 0.9704\n",
      "Epoch 52/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1212 - accuracy: 0.9493 - val_loss: 0.1045 - val_accuracy: 0.9603\n",
      "Epoch 53/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1213 - accuracy: 0.9487 - val_loss: 0.1014 - val_accuracy: 0.9630\n",
      "Epoch 54/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1179 - accuracy: 0.9507 - val_loss: 0.1064 - val_accuracy: 0.9553\n",
      "Epoch 55/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1193 - accuracy: 0.9487 - val_loss: 0.1199 - val_accuracy: 0.9468\n",
      "Epoch 56/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1162 - accuracy: 0.9512 - val_loss: 0.1022 - val_accuracy: 0.9564\n",
      "Epoch 57/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1127 - accuracy: 0.9515 - val_loss: 0.1109 - val_accuracy: 0.9555\n",
      "Epoch 58/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1120 - accuracy: 0.9520 - val_loss: 0.0915 - val_accuracy: 0.9662\n",
      "Epoch 59/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1102 - accuracy: 0.9522 - val_loss: 0.0938 - val_accuracy: 0.9637\n",
      "Epoch 60/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1110 - accuracy: 0.9513 - val_loss: 0.1005 - val_accuracy: 0.9644\n",
      "Epoch 61/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1082 - accuracy: 0.9543 - val_loss: 0.0915 - val_accuracy: 0.9674\n",
      "Epoch 62/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1094 - accuracy: 0.9517 - val_loss: 0.0986 - val_accuracy: 0.9533\n",
      "Epoch 63/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1099 - accuracy: 0.9499 - val_loss: 0.0809 - val_accuracy: 0.9683\n",
      "Epoch 64/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1056 - accuracy: 0.9534 - val_loss: 0.1154 - val_accuracy: 0.9482\n",
      "Epoch 65/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1088 - accuracy: 0.9504 - val_loss: 0.0825 - val_accuracy: 0.9651\n",
      "Epoch 66/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1101 - accuracy: 0.9505 - val_loss: 0.1255 - val_accuracy: 0.9441\n",
      "Epoch 67/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1010 - accuracy: 0.9560 - val_loss: 0.0978 - val_accuracy: 0.9550\n",
      "Epoch 68/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1019 - accuracy: 0.9534 - val_loss: 0.0919 - val_accuracy: 0.9687\n",
      "Epoch 69/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1021 - accuracy: 0.9535 - val_loss: 0.0817 - val_accuracy: 0.9649\n",
      "Epoch 70/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1006 - accuracy: 0.9562 - val_loss: 0.1089 - val_accuracy: 0.9574\n",
      "Epoch 71/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.1093 - accuracy: 0.9505 - val_loss: 0.0864 - val_accuracy: 0.9619\n",
      "Epoch 72/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0973 - accuracy: 0.9559 - val_loss: 0.1110 - val_accuracy: 0.9403\n",
      "Epoch 73/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0990 - accuracy: 0.9550 - val_loss: 0.1170 - val_accuracy: 0.9430\n",
      "Epoch 74/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0983 - accuracy: 0.9549 - val_loss: 0.0860 - val_accuracy: 0.9590\n",
      "Epoch 75/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0986 - accuracy: 0.9554 - val_loss: 0.1062 - val_accuracy: 0.9503\n",
      "Epoch 76/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0985 - accuracy: 0.9544 - val_loss: 0.1293 - val_accuracy: 0.9459\n",
      "Epoch 77/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0962 - accuracy: 0.9556 - val_loss: 0.1149 - val_accuracy: 0.9341\n",
      "Epoch 78/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0959 - accuracy: 0.9578 - val_loss: 0.0783 - val_accuracy: 0.9640\n",
      "Epoch 79/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0987 - accuracy: 0.9535 - val_loss: 0.0849 - val_accuracy: 0.9589\n",
      "Epoch 80/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0931 - accuracy: 0.9576 - val_loss: 0.0997 - val_accuracy: 0.9452\n",
      "Epoch 81/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0931 - accuracy: 0.9584 - val_loss: 0.0728 - val_accuracy: 0.9663\n",
      "Epoch 82/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0968 - accuracy: 0.9567 - val_loss: 0.0742 - val_accuracy: 0.9712\n",
      "Epoch 83/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0910 - accuracy: 0.9585 - val_loss: 0.0769 - val_accuracy: 0.9708\n",
      "Epoch 84/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0917 - accuracy: 0.9592 - val_loss: 0.0724 - val_accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0880 - accuracy: 0.9608 - val_loss: 0.0753 - val_accuracy: 0.9647\n",
      "Epoch 86/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0900 - accuracy: 0.9584 - val_loss: 0.0929 - val_accuracy: 0.9494\n",
      "Epoch 87/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0976 - accuracy: 0.9555 - val_loss: 0.0755 - val_accuracy: 0.9681\n",
      "Epoch 88/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0941 - accuracy: 0.9596 - val_loss: 0.0704 - val_accuracy: 0.9681\n",
      "Epoch 89/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0861 - accuracy: 0.9614 - val_loss: 0.0774 - val_accuracy: 0.9642\n",
      "Epoch 90/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0886 - accuracy: 0.9599 - val_loss: 0.0811 - val_accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0834 - accuracy: 0.9650 - val_loss: 0.0702 - val_accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0893 - accuracy: 0.9600 - val_loss: 0.0687 - val_accuracy: 0.9722\n",
      "Epoch 93/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0855 - accuracy: 0.9626 - val_loss: 0.0692 - val_accuracy: 0.9704\n",
      "Epoch 94/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0850 - accuracy: 0.9614 - val_loss: 0.0744 - val_accuracy: 0.9640\n",
      "Epoch 95/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0919 - accuracy: 0.9597 - val_loss: 0.0653 - val_accuracy: 0.9717\n",
      "Epoch 96/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0820 - accuracy: 0.9641 - val_loss: 0.0700 - val_accuracy: 0.9717\n",
      "Epoch 97/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0848 - accuracy: 0.9619 - val_loss: 0.0895 - val_accuracy: 0.9576\n",
      "Epoch 98/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0834 - accuracy: 0.9621 - val_loss: 0.1161 - val_accuracy: 0.9396\n",
      "Epoch 99/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0897 - accuracy: 0.9594 - val_loss: 0.0723 - val_accuracy: 0.9704\n",
      "Epoch 100/100\n",
      "522/522 [==============================] - 16s 30ms/step - loss: 0.0889 - accuracy: 0.9590 - val_loss: 0.0733 - val_accuracy: 0.9678\n",
      "185/185 [==============================] - 2s 11ms/step - loss: 0.1259 - accuracy: 0.9466\n",
      "Test accuracy: 0.9465830326080322\n",
      "185/185 [==============================] - 2s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Now create train, validation, and test series with appropriate structure\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "################## LSTMs ##################\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(InputLayer((MAX_PROCESSES_LEN, MAX_FEATURES_LEN)))\n",
    "\n",
    "# First LSTM layer with 256 units\n",
    "model1.add(LSTM(256, return_sequences=True))\n",
    "model1.add(Dropout(0.1))  # 10% dropout\n",
    "\n",
    "# Second LSTM layer with 128 units\n",
    "model1.add(LSTM(128, return_sequences=True))\n",
    "model1.add(Dropout(0.1))\n",
    "\n",
    "# Third LSTM layer with 64 units\n",
    "model1.add(LSTM(64))\n",
    "model1.add(Dropout(0.1))\n",
    "\n",
    "# Output layer with softmax activation for binary classification\n",
    "model1.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# TODO: Need to tune the aforementioned and also I am not sure they used the same architecture\n",
    "# TODO: Also maybe use the 120 and prune the ones with more than 120\n",
    "# TODO: Check that the data grouped together by the correct label, or fin another way to group them\n",
    "\n",
    "################## LSTMs ##################\n",
    "\n",
    "################## BiDi ##################\n",
    "\n",
    "\n",
    "\n",
    "# # Define the Bidirectional LSTM model\n",
    "# model1 = Sequential()\n",
    "\n",
    "# model1.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
    "# model1.add(Dropout(0.1))  # 10% dropout for regularization\n",
    "\n",
    "# model1.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "# model1.add(Dropout(0.1))  # 10% dropout for regularization\n",
    "\n",
    "# model1.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "# model1.add(Dropout(0.1))  # 10% dropout for regularization\n",
    "\n",
    "# model1.add(Bidirectional(LSTM(64)))\n",
    "# model1.add(Dropout(0.1))  # 10% dropout for regularization\n",
    "\n",
    "# # Final dense layer with softmax activation for binary classification\n",
    "# model1.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "# # Compile the model\n",
    "# # model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# # Example usage (assuming your data is preprocessed and ready)\n",
    "# # model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "# model1.compile(loss=BinaryCrossentropy(), optimizer=Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "################## BiDi ##################\n",
    "\n",
    "\n",
    "# Train the model with batch size of 32, 40 epochs from the paper\n",
    "history = model1.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model1.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Use classification metrics from scikit-learn\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = model1.predict(x_test)  # Predict class probabilities\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)  # Get class labels (index of max probability)\n",
    "\n",
    "# model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd0b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5897, 2)\n",
      "(5897, 2)\n",
      "[[9.9999917e-01 8.9082067e-07]\n",
      " [9.9999964e-01 3.6738032e-07]\n",
      " [9.9999964e-01 3.3200746e-07]\n",
      " ...\n",
      " [1.4293394e-05 9.9998569e-01]\n",
      " [1.4290463e-05 9.9998569e-01]\n",
      " [1.4291594e-05 9.9998569e-01]]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(5897, 2)\n",
      "(5897, 2)\n",
      "[[9.9999917e-01 8.9082067e-07]\n",
      " [9.9999964e-01 3.6738032e-07]\n",
      " [9.9999964e-01 3.3200746e-07]\n",
      " ...\n",
      " [1.4293394e-05 9.9998569e-01]\n",
      " [1.4290463e-05 9.9998569e-01]\n",
      " [1.4291594e-05 9.9998569e-01]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# print('Y_pred = ', y_pred)\n",
    "\n",
    "print(type(y_test))\n",
    "print(type(y_pred))\n",
    "\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)# print('Y_pred = ', y_pred)\n",
    "\n",
    "print(type(y_test))\n",
    "print(type(y_pred))\n",
    "\n",
    "print(y_test.shape)\n",
    "print(y_pred.shape)\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "# START WITH THE FOLLOWING (28-3-24)\n",
    "# SOSOS: I think the problem is length of the feature vector. I should drop the processes names or use another type of encoding. Also drop their pids or keep just one of these that uniquelly characterize a same-name process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract validation accuracy and loss from history\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Generate epochs list\n",
    "epochs = range(1, len(val_acc) + 1)  # +1 for starting at 1\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.plot(epochs, val_acc, 'bo', label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot validation loss on the same figure\n",
    "plt.plot(epochs, val_loss, 'ro', label='Validation Loss')\n",
    "plt.legend()  # Show both plots in the legend\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
