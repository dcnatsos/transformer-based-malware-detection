import os
import pandas as pd
from sklearn.model_selection import train_test_split
import math

SAMPLE_NO = 8
TRAIN_SIZE = math.floor(SAMPLE_NO*0.6)
VALIDATION_SIZE = math.ceil(SAMPLE_NO*0.2)
TEST_SIZE = math.ceil(SAMPLE_NO*0.2)

if TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE != SAMPLE_NO:
    raise Exception(f"Bad dataset size:\nSample Number: {SAMPLE_NO}\nTrain Size: {TRAIN_SIZE}\nValidation Size: {VALIDATION_SIZE}\nTest Size: {TEST_SIZE}")

# Read each preprocessed CSV file representing an experiment
data_path = os.path.join(os.getcwd(), 'preprocessed')
csv_files = os.listdir(data_path)[:SAMPLE_NO]
dfs = [pd.read_csv(os.path.join(data_path, file)) for file in csv_files] # A list of dfs, 1 csv = 1 df

# Drop specified columns from all dataframes
columns_to_drop = [
'sample_no', 'exp_no', 'vm_id', 'pid', 'ppid', 'process_creation_time', 'gid_real',
'gid_saved', 'mem_pss', #'experiment_id' , 'sample_time', 'label'
] 

# Drop unecessary columns
for df in dfs:
    df.drop(columns=columns_to_drop, inplace=True)

# Split labels into a different df?


# Split data by experiment into training, validation, and testing sets
train_experiments, test_experiments = train_test_split(csv_files, test_size=(TEST_SIZE+VALIDATION_SIZE), random_state=42)
val_experiments, test_experiments = train_test_split(test_experiments, test_size=TEST_SIZE, random_state=42)

print('Train:', train_experiments)
print('Validation', val_experiments)
print('Test:', test_experiments)

# Collect data for each set and transform into series format
datasets = {}
for split, experiments in zip(["train", "val", "test"], [train_experiments, val_experiments, test_experiments]): # (('train', [1, 2]), ('val', [3]), ('test', [4]))
    
    # data contains the concatenated train/validation/test dataframe in the respective iteration (3 iterations)
    data = pd.concat([dfs[csv_files.index(exp)] for exp in experiments]) 
    
    # series_data = []
    # for experiment_id, df in data.groupby('experiment_id'): # Per each dataframe (train,validation,test) group by experiment id
    #     snapshots = df.groupby('sample_time') # And then group by sample time per each experiment
    #     series = []
    #     for timestamp, group in snapshots:
    #         series.append(group.drop(columns=['sample_time', 'experiment_id']).values.tolist())
    #     series_data.append((series, df['label'].iloc[0]))  # Append a tuple containing series data and label
    
    # datasets[split] = series_data

    series_data = []
    for experiment_id, df in data.groupby('experiment_id'): # Per each dataframe (train,validation,test) group by experiment id
        snapshots = df.groupby('sample_time') # And then group by sample time per each experiment
        for timestamp, group in snapshots:

            # Check if all labels within the snapshot are the same
            if group['label'].nunique() > 1:
                raise ValueError("Snapshot contains different labels")
            
            # Determine the class label for the snapshot
            label = 1 if any(group['label']) else 0
            
            # Append a tuple containing series data and label
            series = group.drop(columns=['sample_time', 'experiment_id', 'label']).values.tolist()
            series_data.append((series, label))

    datasets[split] = series_data

    if len(datasets[split]) != pd.concat([dfs[csv_files.index(exp)] for exp in experiments])['sample_time'].unique().size:
        raise ValueError(f"Series sample number are not the same for split: {split}")

    print(f'Split: {split} sample size: ',len(datasets[split]))


print(type(datasets['val']))
print(len(datasets['val']))
print(type(datasets['val'][0]))
print(len(datasets['val'][0]))
print(type(datasets['val'][0][0]))
print(len(datasets['val'][0][0]))
print(type(datasets['val'][0][0][0]))
print(len(datasets['val'][0][0][0]))


# Now create train, validation, and test series with appropriate structure
from tensorflow import keras
from tensorflow.keras.layers import LSTM, Dense, Embedding

train_series = []
for series_data in datasets['train']:
  sequence = series_data[0]  # Access resource usage values
  label = series_data[1]  # Access class label
  train_series.append((sequence, label))

val_series = []  # Create validation series similarly
test_series = []  # Create test series similarly

# Extract features (sequences) and labels from series
train_X, train_y = zip(*train_series)
val_X, val_y = zip(*val_series)
test_X, test_y = zip(*test_series)

# Determine vocabulary size (assuming all features are numerical)
vocabulary_size = len(set([item for sublist in train_X for item in sublist])) + 1  # +1 for padding

# Define the RNN model (replace with your desired architecture)
model = keras.Sequential([
  Embedding(vocabulary_size, embedding_dim=16, input_length=120),  # Adjust embedding dimension if needed
  LSTM(64, return_sequences=True),  # Adjust number of units
  LSTM(32),
  Dense(1, activation='sigmoid')  # For binary classification
])

# Compile the model with specified learning rate
model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])

# Train the model with batch size of 32
model.fit(train_X, train_y, epochs=10, batch_size=32, validation_data=(val_X, val_y))  # Adjust epochs as needed

# Evaluate the model on test data
model.evaluate(test_X, test_y)


# print(datasets['val'][0])

# Now each element within datasets['train'/'val'/'test'] contains a list of N 2x1 tuples, where N is the number of samples aka series.
# Each tuple contains in its first place (tuple[0]) a list containing the number of processes at the given timestamp. This list has a maximum
# length of 120 (rows) where each row is a list of numericals, each numerical representing the value of a feature for the process.
# The second place of the tuple (tuple[1]) contains the class label of the series as a numerical.



# from tensorflow import keras
# from tensorflow.keras.layers import LSTM, Dense, Embedding
# # from tensorflow.keras.preprocessing.sequence import pad_sequence
# import tensorflow as tf

# # Assuming your datasets dictionary holds training, validation and test data

# # Define the function to create series with padding
# def create_series(df, max_len=120):
#     series_data = []
#     for experiment_id, group in df.groupby('experiment_id'):
#         snapshots = group.groupby('sample_time')
#         for timestamp, snapshot in snapshots:
#             # Check for different labels
#             if snapshot['label'].nunique() > 1:
#                 raise ValueError("Snapshot contains different labels")
#             label = 1 if any(snapshot['label']) else 0
#             sequence = snapshot.drop(columns=['sample_time', 'experiment_id', 'label']).values.tolist()
#             # Pad or truncate sequence to max_len
#             padded_sequence = tf.keras.utils.pad_sequences(sequence, max_len=max_len, padding='post', value=0.0)  # Pad with zeros at the end
#             series_data.append((padded_sequence, label))
#     return series_data

# # Create series for train, validation and test data with padding
# train_series = create_series(datasets['train'], max_len=120)
# val_series = create_series(datasets['val'], max_len=120)
# test_series = create_series(datasets['test'], max_len=120)

# # Extract features (sequences) and labels from series
# train_X, train_y = zip(*train_series)
# val_X, val_y = zip(*val_series)
# test_X, test_y = zip(*test_series)

# # Determine vocabulary size (assuming all features are numerical)
# # You might need to adjust this based on your data
# vocabulary_size = len(set([item for sublist in train_X for item in sublist])) + 1  # +1 for padding

# max_len = 120 # ???

# # Define the RNN model (replace with your desired architecture)
# model = keras.Sequential([
#     Embedding(vocabulary_size, embedding_dim=16, input_length=max_len),  # Adjust embedding dimension if needed
#     LSTM(64, return_sequences=True),  # Adjust number of units
#     LSTM(32),
#     Dense(1, activation='sigmoid')  # For binary classification
# ])

# # Compile the model with specified learning rate
# model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])

# # Train the model with batch size of 32
# model.fit(train_X, train_y, epochs=10, batch_size=32, validation_data=(val_X, val_y))  # Adjust epochs as needed

# # Evaluate the model on test data
# model.evaluate(test_X, test_y)


################################# Debug #################################
# import csv

# output_filename = "val_series_data.csv"
# series_data = datasets['val'][0][0]  # Get the series data from the first tuple in datasets['val']
# label = datasets['val'][0][1]  # Get the label from the first tuple in datasets['val']

# with open(output_filename, 'w', newline='') as csvfile:
#     writer = csv.writer(csvfile)
    
#     # Write the header row
#     writer.writerow(["timestamp"] + [f"feature_{i}" for i in range(len(series_data[0]))] + ["label"])
    
#     # Write each data point
#     for timestamp, data_point in enumerate(series_data):
#         writer.writerow([timestamp] + data_point + [label])

# print(f"Series data written to {output_filename}")

################################# Debug #################################
# output_folder = "series_data_csv"

# # Create the output folder if it doesn't exist
# os.makedirs(output_folder, exist_ok=True)

# for i, (series, label) in enumerate(series_data):
#     output_filename = os.path.join(output_folder, f"series_{i}.csv")
#     with open(output_filename, 'w') as f:
#         f.write("timestamp," + ",".join([f"feature_{j}" for j in range(len(series[0]))]) + ",label\n")
#         for timestamp, data_point in enumerate(series):
#             f.write(f"{timestamp}," + ",".join(map(str, data_point)) + f",{label}\n")
#     print(f"Series data written to {output_filename}")
################################# Debug #################################


############################### RNN Train ###############################


# Split the datasets into training, validation, and test sets
# train_data = datasets['train']
# val_data = datasets['val']
# test_data = datasets['test']

# # Unpack the data
# x_train, y_train = zip(*train_data)
# x_val, y_val = zip(*val_data)
# x_test, y_test = zip(*test_data)

# print(type(y_train))
# print(len(y_train))
# print(y_train)

# Convert data to numpy arrays
# import numpy as np
# x_train = np.array(x_train)
# y_train = np.array(y_train)
# x_val = np.array(x_val)
# y_val = np.array(y_val)
# x_test = np.array(x_test)
# y_test = np.array(y_test)

# # Define the maximum sequence length
# max_sequence_length = x_train.shape[1]

# # Define the RNN model
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# embedding_dim = 100  # Example embedding dimension
# num_classes = 2  # Assuming binary classification

# model = Sequential([
#     Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_sequence_length),
#     SimpleRNN(units=64, activation='tanh'),  # Adjust units and activation function as needed
#     Dense(units=num_classes, activation='softmax')
# ])

# # Compile the model
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Train the model
# model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))

# # Evaluate on test data
# test_loss, test_accuracy = model.evaluate(x_test, y_test)
# print("Test Accuracy:", test_accuracy)












# x_val = [data[0] for data in val_data]
# y_val = [data[1] for data in val_data]

# x_test = [data[0] for data in test_data]
# y_test = [data[1] for data in test_data]

# # Preprocess the data (padding sequences if necessary)
# max_sequence_length = 120  # Set the maximum sequence length to 120
# x_train = pad_sequences(x_train, maxlen=max_sequence_length, padding='post')
# x_val = pad_sequences(x_val, maxlen=max_sequence_length, padding='post')
# x_test = pad_sequences(x_test, maxlen=max_sequence_length, padding='post')

# # Define the RNN model
# model = Sequential([
#     Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_sequence_length),
#     SimpleRNN(units=64, activation='tanh', return_sequences=False),  # You can also try LSTM or GRU here
#     Dense(units=num_classes, activation='softmax')
# ])

# # Compile the model
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Train the model
# model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))

# # Evaluate on test data
# test_loss, test_accuracy = model.evaluate(x_test, y_test)
# print("Test Accuracy:", test_accuracy)



# Step 3: Create the sequences

# Step 4: Split into train, validation and test
    
# Step 5: Train and tune the model
    
# Step 6: Test the model