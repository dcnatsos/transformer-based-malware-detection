import sys
import random

python3_path = sys.executable
print("Python 3 interpreter being used:", python3_path)

import os
import pandas as pd
from sklearn.model_selection import train_test_split
import math
import numpy as np
import keras
from keras import layers
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import joblib
import time
import pickle
from multiprocessing import Manager, Process
from keras.callbacks import Callback

# Define dataGenerator params
DATA_BATCH_SIZE = 1  # Adjust as needed
NUMBER_OF_BATCHES = 5  # Adjust as needed
BATCH_FILES_DIR = 'augmented_data_3'  # Adjust as needed

# Directory to save checkpoints
CHECKPOINT_DIR = 'checkpoints/transformer_vs_lstm/variable_dataset_len/10_1'
FILE_NAME = 'experiment1_200epochs_patience20_dataset_10_1.pkl'
IMAGE_DIR = 'images/transformer_vs_lstm/variable_dataset_len/10_1'
BEST_TRANSFORMER_CPT = "best_model_TRANSFORMER.keras"
BEST_LSTM_CPT = "best_model_LSTM.keras"

MAX_PROCESSES_LEN = 227 # The paper says 120 but there are some with 190
MAX_FEATURES_LEN = 34 # This is the number of features per process 

""" 1. Define the data generator to import the files sequentially and feed them to the model during training. (This step replaces the data loading step) """

class DataGenerator(keras.utils.Sequence):
    def __init__(self, batch_files, batch_sizes, batch_size):
        self.batch_files = batch_files
        self.batch_sizes = batch_sizes
        self.batch_size = batch_size
        self.file_index = 0
        self.current_index = 0
        self.current_batch_data = []
        self.load_current_batch()

    def __len__(self):
        total_samples = sum(self.batch_sizes)
        return math.ceil(total_samples / self.batch_size)

    def load_current_batch(self):
        filename = self.batch_files[self.file_index]
        with open(filename, 'rb') as f:
            self.current_batch_data = joblib.load(f)
        self.current_index = 0

    def __getitem__(self, index):
        while index * self.batch_size >= len(self.current_batch_data):
            index -= len(self.current_batch_data) // self.batch_size
            self.file_index = (self.file_index + 1) % len(self.batch_files)
            self.load_current_batch()

        start_index = index * self.batch_size
        end_index = start_index + self.batch_size
        x_data, y_data = self.tuple_dataset_to_X_y(self.current_batch_data[start_index:end_index])
        return x_data, y_data

    def tuple_dataset_to_X_y(self, batch_data):
        x_data = [item[0] for item in batch_data]
        y_data = [item[1] for item in batch_data]
        return np.array(x_data), np.array(y_data)

    def on_epoch_end(self):
        self.file_index = 0
        self.load_current_batch()

class EpochCheckpoint(Callback):
    def __init__(self, model_name, checkpoint_dir, history, metrics_dict):
        super(EpochCheckpoint, self).__init__()
        self.model_name = model_name
        self.checkpoint_dir = checkpoint_dir
        self.history = history
        self.metrics_dict = metrics_dict

    def on_epoch_end(self, epoch, logs=None):
        if logs is not None:
            for key, value in logs.items():
                self.history[key].append(value)
        checkpoint_path = os.path.join(self.checkpoint_dir, f'epoch_{epoch + 1:03d}.h5')
        self.model.save(checkpoint_path)
        save_checkpoint(self.checkpoint_dir, (self.metrics_dict, self.history))

def save_checkpoint(checkpoint_dir, data):
    filename = os.path.join(checkpoint_dir, 'training_state.pkl')
    with open(filename, 'wb') as f:
        pickle.dump(data, f)

# Transformer model definition
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(res)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return x + res

def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0, n_classes=2):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)
    x = layers.GlobalAveragePooling1D(data_format="channels_last")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation="softmax")(x)
    return keras.Model(inputs, outputs)

def evaluate_model_transformer(params, train_generator, val_generator, checkpoint_dir, model_name):
    model = build_model(
        input_shape=(MAX_PROCESSES_LEN, MAX_FEATURES_LEN),
        head_size=params['head_size'],
        num_heads=params['num_heads'],
        ff_dim=params['ff_dim'],
        num_transformer_blocks=params['num_transformer_blocks'],
        mlp_units=params['mlp_units'],
        mlp_dropout=params['dropout'],
        dropout=params['dropout'],
    )

    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),
        metrics=["sparse_categorical_accuracy"],
    )

    history = {k: [] for k in ['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy']}
    metrics_dict = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'training_time': []}

    callbacks = [
        keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, BEST_TRANSFORMER_CPT), save_best_only=True, monitor="val_loss"),
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['patience'], restore_best_weights=True),
        EpochCheckpoint(model_name, checkpoint_dir, history, metrics_dict)
    ]

    start_time = time.time()
    history = model.fit(
        train_generator,
        epochs=params['epochs'],
        steps_per_epoch=len(train_generator),
        callbacks=callbacks,
        validation_data=val_generator,
        validation_steps=len(val_generator),
        verbose=2
    )
    end_time = time.time()
    training_time = end_time - start_time

    val_generator_iter = val_generator.__getitem__
    y_true, y_pred = [], []
    for i in range(len(val_generator)):
        x_val, y_val = val_generator_iter(i)
        y_true.extend(y_val)
        y_pred_batch = model.predict(x_val, verbose=2)
        y_pred.extend(np.argmax(y_pred_batch, axis=-1))

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return accuracy, precision, recall, f1, history.history, model, training_time

def run_evaluation_transformer(params, train_generator, val_generator, return_dict, models_rnn, checkpoint_dir, model_name):
    accuracy, precision, recall, f1, history, model, training_time = evaluate_model_transformer(params, train_generator, val_generator, checkpoint_dir, model_name)
    return_dict['accuracy'] = accuracy
    return_dict['precision'] = precision
    return_dict['recall'] = recall
    return_dict['f1'] = f1
    return_dict['history'] = history
    return_dict['training_time'] = training_time
    models_rnn.append(model)
    keras.backend.clear_session()  # Free GPU VRAM

def evaluate_model_lstm(params, train_generator, val_generator, checkpoint_dir, model_name):
    model = Sequential()
    model.add(InputLayer((MAX_PROCESSES_LEN, MAX_FEATURES_LEN)))
    model.add(LSTM(256, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(LSTM(64))
    model.add(Dropout(0.1))
    model.add(Dense(2, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy", optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']), metrics=['sparse_categorical_accuracy'])

    history = {k: [] for k in ['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy']}
    metrics_dict = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'training_time': []}

    callbacks = [
        keras.callbacks.ModelCheckpoint(os.path.join(checkpoint_dir, BEST_LSTM_CPT), save_best_only=True, monitor="val_loss"),
        keras.callbacks.EarlyStopping(monitor='val_loss', patience=params['patience'], restore_best_weights=True),
        EpochCheckpoint(model_name, checkpoint_dir, history, metrics_dict)
    ]

    start_time = time.time()
    history = model.fit(
        train_generator,
        epochs=params['epochs'],
        steps_per_epoch=len(train_generator),
        callbacks=callbacks,
        validation_data=val_generator,
        validation_steps=len(val_generator),
        verbose=2
    )
    end_time = time.time()
    training_time = end_time - start_time

    val_generator_iter = val_generator.__getitem__
    y_true, y_pred = [], []
    for i in range(len(val_generator)):
        x_val, y_val = val_generator_iter(i)
        y_true.extend(y_val)
        y_pred_batch = model.predict(x_val, verbose=2)
        y_pred.extend(np.argmax(y_pred_batch, axis=-1))

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    return accuracy, precision, recall, f1, history.history, model, training_time

def run_evaluation_lstm(params, train_generator, val_generator, return_dict, models_rnn, checkpoint_dir, model_name):
    accuracy, precision, recall, f1, history, model, training_time = evaluate_model_lstm(params, train_generator, val_generator, checkpoint_dir, model_name)
    return_dict['accuracy'] = accuracy
    return_dict['precision'] = precision
    return_dict['recall'] = recall
    return_dict['f1'] = f1
    return_dict['history'] = history
    return_dict['training_time'] = training_time
    models_rnn.append(model)
    keras.backend.clear_session()  # Free GPU VRAM


""" 3. Define the model parameters """


# TODO: Adjust as needed
EPOCHS = 100
PATIENCE = 10

TRANSFORMER_MODEL = {
    'num_transformer_blocks': 4,
    'head_size': 128,
    'num_heads': 6,
    'ff_dim': 512,
    'learning_rate': 0.001,
    'mlp_units': [128],
    'dropout': 0.1,
    'batch_size': 96,
    'epochs': EPOCHS,
    'patience': PATIENCE
}

LSTM_MODEL = {
    'batch_size': 16,
    'learning_rate': 1e-05,
    'epochs': EPOCHS,
    'patience': PATIENCE
}

MODELS = {
    'TRANSFORMER': TRANSFORMER_MODEL,
    'LSTM': LSTM_MODEL
}


""" 4. Train the models while storing checkpoints """


os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(os.path.join(CHECKPOINT_DIR, 'transformer'), exist_ok=True)
os.makedirs(os.path.join(CHECKPOINT_DIR, 'lstm'), exist_ok=True)

# Function to load checkpoints
def load_checkpoint(filename):
    if os.path.exists(os.path.join(CHECKPOINT_DIR, filename)):
        with open(os.path.join(CHECKPOINT_DIR, filename), 'rb') as f:
            return pickle.load(f)
    return None

# Collect metrics for both models
model_metrics = {key: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'training_time': []} for key in MODELS.keys()}
histories = {key: [] for key in MODELS.keys()}

# Load saved state if available
saved_state = load_checkpoint(FILE_NAME)
if saved_state:
    model_metrics, histories = saved_state

manager = Manager()
return_dict = manager.dict()
manager2 = Manager()
models_rnn = manager2.list()

batch_files = [f'{BATCH_FILES_DIR}/batch_{i}.pkl' for i in range(NUMBER_OF_BATCHES)]
batch_sizes = [57330, 57330, 54770, 58430, 54330]  # Hardcoded sample sizes

train_files = batch_files[1:]
train_sizes = batch_sizes[1:]
val_file = batch_files[0]
val_size = batch_sizes[0]

for key, params in MODELS.items():
    keras.backend.clear_session()
    print(f'Training {key} with parameters:', params)
    time.sleep(5)

    train_generator = DataGenerator(train_files, train_sizes, batch_size=params['batch_size'])
    val_generator = DataGenerator([val_file], [val_size], batch_size=params['batch_size'])

    model_checkpoint_dir = os.path.join(CHECKPOINT_DIR, key.lower())
    os.makedirs(model_checkpoint_dir, exist_ok=True)

    if key == 'TRANSFORMER':
        p = Process(target=run_evaluation_transformer, args=(params, train_generator, val_generator, return_dict, models_rnn, model_checkpoint_dir, key))
    elif key == 'LSTM':
        p = Process(target=run_evaluation_lstm, args=(params, train_generator, val_generator, return_dict, models_rnn, model_checkpoint_dir, key))

    p.start()
    p.join()  # Wait for the subprocess to finish

    accuracy = return_dict['accuracy']
    precision = return_dict['precision']
    recall = return_dict['recall']
    f1 = return_dict['f1']
    history = return_dict['history']
    training_time = return_dict['training_time']

    model_metrics[key]['accuracy'].append(accuracy)
    model_metrics[key]['precision'].append(precision)
    model_metrics[key]['recall'].append(recall)
    model_metrics[key]['f1'].append(f1)
    model_metrics[key]['training_time'].append(training_time)
    histories[key].append(history)

save_checkpoint(FILE_NAME, (model_metrics, histories))

import matplotlib.pyplot as plt

for key, fold_histories in histories.items():
    metric = "sparse_categorical_accuracy"

    BATCH_SIZE = MODELS[key]['batch_size']
    LEARNING_RATE = MODELS[key]['learning_rate']
    PARAMS = MODELS[key].copy()
    PARAMS.pop('batch_size', None)
    PARAMS.pop('learning_rate', None)
    
    for fold_index, history in enumerate(fold_histories):
        plt.figure()
        plt.plot(history['sparse_categorical_accuracy'])
        plt.plot(history["val_sparse_categorical_accuracy"])
        plt.title(f"Sparse Categorical Accuracy {key.lower()} - Fold {fold_index+1}")
        plt.ylabel("Sparse Categorical Accuracy", fontsize="large")
        plt.xlabel("Epoch", fontsize="large")
        plt.legend(["Train", "Validation"], loc="best")
        plt.savefig(f'{IMAGE_DIR}/training_{key.lower()}_accuracy_fold{fold_index+1}_epochs{EPOCHS}_BS{BATCH_SIZE}_LR{LEARNING_RATE}.jpg')
        plt.show()
        plt.close()

    metric = "loss"

    for fold_index, history in enumerate(fold_histories):
        plt.figure()
        plt.plot(history['loss'])
        plt.plot(history["val_loss"])
        plt.title(f"Training and Validation Loss {key.lower()} - Fold {fold_index+1}")
        plt.ylabel("Loss", fontsize="large")
        plt.xlabel("Epoch", fontsize="large")
        plt.legend(["Train", "Validation"], loc="best")
        plt.savefig(f'{IMAGE_DIR}/training_{key.lower()}_loss_fold{fold_index+1}_epochs{EPOCHS}_BS{BATCH_SIZE}_LR{LEARNING_RATE}.jpg')
        plt.show()
        plt.close()

from statistics import mean

for model_name, metrics in model_metrics.items():
    print(f"\n{model_name} metrics:")
    print('Accuracy:', model_metrics[model_name]['accuracy'])
    print('Precision:', model_metrics[model_name]['precision'])
    print('Recall:', model_metrics[model_name]['recall'])
    print('F1-score:', model_metrics[model_name]['f1'])
    print('Training Time (s):', metrics['training_time'])

import statistics

for model_name, metrics in model_metrics.items():
    print(f"\n{model_name} average metrics:")

    accuracy_mean = statistics.mean(metrics['accuracy'])
    accuracy_median = statistics.median(metrics['accuracy'])
    accuracy_std = statistics.stdev(metrics['accuracy'])
    print('Accuracy: Mean =', accuracy_mean, 'Median =', accuracy_median, 'Std =', accuracy_std)

    precision_mean = statistics.mean(metrics['precision'])
    precision_median = statistics.median(metrics['precision'])
    precision_std = statistics.stdev(metrics['precision'])
    print('Precision: Mean =', precision_mean, 'Median =', precision_median, 'Std =', precision_std)

    recall_mean = statistics.mean(metrics['recall'])
    recall_median = statistics.median(metrics['recall'])
    recall_std = statistics.stdev(metrics['recall'])
    print('Recall: Mean =', recall_mean, 'Median =', recall_median, 'Std =', recall_std)

    f1_mean = statistics.mean(metrics['f1'])
    f1_median = statistics.median(metrics['f1'])
    f1_std = statistics.stdev(metrics['f1'])
    print('F1-score: Mean =', f1_mean, 'Median =', f1_median, 'Std =', f1_std)

    training_time_mean = statistics.mean(metrics['training_time'])
    training_time_median = statistics.median(metrics['training_time'])
    training_time_std = statistics.stdev(metrics['training_time'])
    print('Average Training Time (s): Mean =', training_time_mean, 'Median =', training_time_median, 'Std =', training_time_std)

def find_first_less_than(arr, threshold):
    return next((i for i, value in enumerate(arr) if value < threshold), EPOCHS)

def find_first_greater_than(arr, threshold):
    return next((i for i, value in enumerate(arr) if value > threshold), EPOCHS)

for model in histories.keys():
    val_index_loss = 0
    val_index_acc = 0
    for fold in histories[model]:
        temp = find_first_less_than(fold['val_loss'], 0.1)
        val_index_loss += temp
        temp = find_first_greater_than(fold['val_sparse_categorical_accuracy'], 0.95)
        val_index_acc += temp

    print(f'\nAverage Index for {model} where val_loss < 0.1:', val_index_loss / len(histories[model]))
    print(f'Average Index for {model} where val_acc > 0.95:', val_index_acc / len(histories[model]))
