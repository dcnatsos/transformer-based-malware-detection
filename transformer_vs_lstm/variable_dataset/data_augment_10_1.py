import math
import os
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Define samples factor for decreasing dataset length
SAMPLES_FACTOR = 10 # 100
NOISE_LEVEL = 0.01

# Define Training, Validation, Test sizes
SAMPLE_NO = 104 #8
TRAIN_SIZE = math.floor(SAMPLE_NO*0.6)
VALIDATION_SIZE = math.ceil(SAMPLE_NO*0.2)
TEST_SIZE = math.ceil(SAMPLE_NO*0.2)

if TRAIN_SIZE + VALIDATION_SIZE + TEST_SIZE != SAMPLE_NO:
    raise Exception(f"Bad dataset size:\nSample Number: {SAMPLE_NO}\nTrain Size: {TRAIN_SIZE}\nValidation Size: {VALIDATION_SIZE}\nTest Size: {TEST_SIZE}")

# Read each preprocessed CSV file representing an experiment
data_path = os.path.join(os.getcwd(), 'preprocessed')
csv_files = os.listdir(data_path)[:SAMPLE_NO] # Use only the first SAMPLE_NO (for testing the training.py)
dfs = [pd.read_csv(os.path.join(data_path, file)) for file in csv_files] # A list of dfs, 1 csv = 1 df

# Drop specified columns from all dataframes
columns_to_drop = [
'sample_no', 'exp_no', 'vm_id', 'pid', 'ppid', 'process_creation_time', 'gid_real',
'gid_saved', 'mem_pss', #'experiment_id' , 'sample_time', 'label' # These three are dropped later
] 

# Drop unecessary columns
for df in dfs:
    df.drop(columns=columns_to_drop, inplace=True)
    df.drop(columns=[s for s in df.columns if s.startswith("name_")], inplace=True) # DROP THE 'name' feature

print(dfs[0].columns)
print(len(dfs[0].columns))

# Split data by experiment into training, validation, and testing sets
train_experiments, test_experiments = train_test_split(csv_files, test_size=(TEST_SIZE+VALIDATION_SIZE), random_state=42)
val_experiments, test_experiments = train_test_split(test_experiments, test_size=TEST_SIZE, random_state=42)

print('Train:', train_experiments)
print('Validation', val_experiments)
print('Test:', test_experiments)

# Collect data for each set and transform into series format
datasets = {}
for split, experiments in zip(["train", "val", "test"], [train_experiments, val_experiments, test_experiments]): # (('train', [1, 2]), ('val', [3]), ('test', [4]))
    
    # data contains the concatenated train/validation/test dataframe in the respective iteration (3 iterations)
    data = pd.concat([dfs[csv_files.index(exp)] for exp in experiments]) 

    series_data = []
    for experiment_id, df in data.groupby('experiment_id'): # Per each dataframe (train, validation, test) group by experiment id
        snapshots = df.groupby('sample_time') # And then group by sample time per each experiment
        for timestamp, group in snapshots:

            # Check if all labels within the snapshot are the same
            if group['label'].nunique() > 1:
                raise ValueError("Snapshot contains different labels")
            
            # Determine the class label for the snapshot
            label = 1 if any(group['label']) else 0
            
            # Append a tuple containing series data and label
            series = group.drop(columns=['sample_time', 'experiment_id', 'label']).values.tolist()
            series_data.append((series, label))

    datasets[split] = series_data

    if len(datasets[split]) != pd.concat([dfs[csv_files.index(exp)] for exp in experiments])['sample_time'].unique().size:
        raise ValueError(f"Series sample number are not the same for split: {split}")

    print(f'Split: {split} sample size: ', len(datasets[split]))


"""
    - datasets -> dict: A dictionary containing the data splitted in training, validation, test set
    - datasets.train/val/test -> list: A list of tuples, each one containing:
    - datasets.train/val/test[0] -> tuple: Tuple i
    - datasets.train/val/test[i][0] -> list: snapshot/subsamples list of the i-th tuple/snapshot
    - datasets.train/val/test[i][1] -> int: label of the i-th tuple/snapshot
    - datasets.train/val/test[i][j][0] -> int: feature vector of the j-th process and the i-th tuple/snapshot

    -- Each tuple refers to a specific snapshot on a given timestamp of the system processes. The label of the tuple
    states whether the system was infected at that time or not. 
    -- (the concept of experiment_id is used up until splitting
    into train/validation/test where all the snapshots from the same experiment are maintained within one of train/val/test
    and not splitted. After that, the experiment_id concept is deprecated)
    -- The infectious process is probably the one that has the same name with the csv -> TODO: Implement this later.
"""

print('Datasets keys: ', datasets.keys())
print(type(datasets['val'])) # Validation List
print(len(datasets['val']))
print(type(datasets['val'][0])) # A snapshot/sample tuple (subsamples list, label)
print(len(datasets['val'][0]))
print(type(datasets['val'][0][0])) # Subsamples list
print(len(datasets['val'][0][0]))
print(type(datasets['val'][0][0][0])) # Feature vector of a specific process at a specific timestamp/snapshot/sample
print(len(datasets['val'][0][0][0]))
datasets['train'][0][0]

# For reproducability, Each call to add_noise generates different noise due to the continuous state 
# of the random number generator.
np.random.seed(42) 

def add_noise(data, noise_level=0.01):
    """
    Adds random noise to the data.
    
    Parameters:
    data (list of lists): The input data to which noise will be added.
    noise_level (float): The standard deviation of the Gaussian noise to be added.

    Returns:
    list of lists: The data with added noise.
    """
    return data + noise_level * np.random.normal(size=data.shape)

def augment_time_series(data, factor, noise_level=0.01):
    """
    Augments the time series data by adding random noise and retains the original data.

    Parameters:
    data (list of tuples): The input time series data.
    factor (int): The factor by which to augment the data.
    noise_level (float): The standard deviation of the Gaussian noise to be added.

    Returns:
    list of tuples: The augmented time series data including original and noisy data.
    """
    augmented_data = data.copy()  # Start with original data
    for _ in range(factor - 1):  # Add noisy data `factor - 1` times
        for series, label in data:
            noisy_series = [add_noise(np.array(snapshot), noise_level) for snapshot in series]
            augmented_data.append((noisy_series, label))
    return augmented_data


# Augment the dataset by adding noise
datasets['train'] = augment_time_series(datasets['train'], SAMPLES_FACTOR, NOISE_LEVEL)
datasets['val'] = augment_time_series(datasets['val'], SAMPLES_FACTOR, NOISE_LEVEL)
datasets['test'] = augment_time_series(datasets['test'], SAMPLES_FACTOR, NOISE_LEVEL)

print(f"New train size: {len(datasets['train'])}")
print(f"New validation size: {len(datasets['val'])}")
print(f"New test size: {len(datasets['test'])}")

MAX_PROCESSES_LEN = 227 # The paper says 120 but there are some with 190
MAX_FEATURES_LEN = 34 # This is the number of features per process 


def prepare_data(data):
    """
    Prepares data for LSTM training.

    Args:
      data: A list of tuples containing (series, label).
          - series: A list of lists, where each inner list represents features 
                    of all processes at a specific timestamp within the snapshot.
          - label: An integer representing the class label (0 or 1) for the entire snapshot.

    Returns:
      A tuple containing two numpy arrays:
          - X: Input data for the LSTM, shaped (samples, feature_dim).
          - y: Labels, shaped (samples,).
    """
    X, y = [], []
    for series, label in data:
        # Get the number of features per process (assuming all processes have the same)
        feature_dim = len(series[0])
        # The entire snapshot is considered as a single sequence
        X.append(series)
        y.append(label)

    # Convert to numpy arrays for efficient processing
    return np.array(X), np.array(y)

## The idea if to have x_train_i = [[[f1], [f2], [f3], ... , [fn]]] y_train_i = [[label_i]], f1 = feature vector of process 1 of snapshot i
## This is multivariate timeseries

## Single-variate (easily scaled to multi-variate): https://www.youtube.com/watch?v=c0k-YLQGKjY&ab_channel=GregHogg
## Multivariate (part 2) at: https://www.youtube.com/watch?v=kGdbPnMCdOg&ab_channel=GregHogg (26:00+)


def tuple_dataset_to_X_y(dataset, one_hot_encode=False):
    X = []
    y = []
    for i in range(len(dataset)): # -1 ???
        row = dataset[i][0] # the i-th snapshot [[f0] [f1] [f2] ... [fn]] (vector of feature vectors/ aka multivariate timeseries)
        
        # Padding, 120 is the maximum number of processes as defined by the paper, but 131 is in the real dataset, and the minimum is 1
        while len(row) < MAX_PROCESSES_LEN:
            row.append(np.zeros(len(row[0])))

        X.append(row)
        label = dataset[i][1]
        y.append(label)

    # One hot encode labels if required
    if one_hot_encode:
        y_np = np.array(y)
        y = np.eye(2)[y_np]

    length = len(X[0])
    print('Length X:', len(X))
    for i in range(len(X)):
        if len(X[i]) != length:
            raise Exception(f'Length X[{i}]: {len(X[i])} is different than length: {length}')

    return np.array(X), np.array(y)

x_train, y_train = tuple_dataset_to_X_y(datasets['train'])
x_val, y_val = tuple_dataset_to_X_y(datasets['val'])
x_test, y_test = tuple_dataset_to_X_y(datasets['test'])

# Free RAM
del datasets

print('Data shapes')
print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)
print(x_test.shape)
print(y_test.shape)
print(y_train)

x_data = np.concatenate((x_train, x_val, x_test), axis=0)

del x_train
del x_val
del x_test

y_data = np.concatenate((y_train, y_val, y_test), axis=0)

del y_train
del y_val
del y_test

# Print the percentage of positive/negative values
print("\nRegarding the augmented dataset:")
print(f'The malicious samples are {len(y_data[y_data == 1])} out of {len(y_data)} total samples: {len(y_data[y_data == 1])/len(y_data)}')


# Define directories to save the numpy arrays
X_DIR = 'data/X'
Y_DIR = 'data/y'

os.makedirs(X_DIR, exist_ok=True)
os.makedirs(Y_DIR, exist_ok=True)

# Save the datasets
for i in range(len(x_data)):
    np.save(os.path.join(X_DIR, f"{i}.npy"), x_data[i])
    np.save(os.path.join(Y_DIR, f"{i}.npy"), y_data[i])

print("Samples and labels saved successfully.")

raise Exception('Stop here')
