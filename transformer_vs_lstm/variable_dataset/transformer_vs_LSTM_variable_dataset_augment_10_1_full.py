import sys

python3_path = sys.executable
print("Python 3 interpreter being used:", python3_path)

import os
import numpy as np
import keras
from keras import layers
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import pickle

from stanford_data_generator import DataGenerator

# Directory to save checkpoints
CHECKPOINT_DIR = '../checkpoints/transformer_vs_lstm/variable_dataset_len/10_1'
FILE_NAME = '../experiment1_200epochs_patience20_dataset_10_1.pkl'
IMAGE_DIR = '../images/transformer_vs_lstm/variable_dataset_len/10_1'
BEST_TRANSFORMER_CPT = "best_model_TRANSFORMER.keras"
BEST_LSTM_CPT = "best_model_LSTM.keras"
DATA_FOLDER = "/home/issel/Workspace/transformer-based-malware-detection/data"

MAX_PROCESSES_LEN = 227 # The paper says 120 but there are some with 190
MAX_FEATURES_LEN = 34 # This is the number of features per process 
NUM_SAMPLES = 282130

EPSILON = 1e-6

# Define metrics
from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(res)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
    n_classes=2
):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format="channels_last")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation="softmax")(x)
    return keras.Model(inputs, outputs)

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import KFold
from scipy.stats import ttest_rel, wilcoxon
import os
from multiprocessing import Process, Manager
import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from sklearn.model_selection import ParameterGrid
import numpy as np


EPOCHS = 50
PATIENCE = 5
FOLDS = 10


def evaluate_model_transformer(params, training_generator: DataGenerator, validation_generator: DataGenerator):
    input_shape = training_generator.getDim() #x_train.shape[1:]

    model = build_model(
        input_shape,
        head_size=params['head_size'],
        num_heads=params['num_heads'],
        ff_dim=params['ff_dim'],
        num_transformer_blocks=params['num_transformer_blocks'],
        mlp_units=params['mlp_units'],
        mlp_dropout=params['dropout'],
        dropout=params['dropout'],
    )

    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),
        metrics=["sparse_categorical_accuracy", precision_m, recall_m, f1_m],
    )

    callbacks = [
        keras.callbacks.ModelCheckpoint(
            BEST_TRANSFORMER_CPT, save_best_only=True, monitor="val_loss"
        ),
        keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=params['patience'], restore_best_weights=True
        ),
    ]

    start_time = time.time()
    history = model.fit(
        training_generator,
        epochs=params['epochs'],
        batch_size=params['batch_size'],
        callbacks=callbacks,
        validation_data=validation_generator,
        verbose=2,
        use_multiprocessing=True,
        workers=10
    )
    end_time = time.time()
    
    training_time = end_time - start_time

    _, accuracy, precision, recall, f1 = model.evaluate(validation_generator, verbose=2)
    # y_pred = model.predict(validation_generator, verbose=2)
    # y_pred_classes = np.argmax(y_pred, axis=-1)

    # accuracy = accuracy_score(y_test, y_pred_classes)
    # precision = precision_score(y_test, y_pred_classes)
    # recall = recall_score(y_test, y_pred_classes)
    # f1 = f1_score(y_test, y_pred_classes)

    return accuracy, precision, recall, f1, history.history, model, training_time

def run_evaluation_transformer(params, training_generator, validation_generator, return_dict, models_rnn):
    accuracy, precision, recall, f1, history, model, training_time = evaluate_model_transformer(params, training_generator, validation_generator)
    return_dict['accuracy'] = accuracy
    return_dict['precision'] = precision
    return_dict['recall'] = recall
    return_dict['f1'] = f1
    return_dict['history'] = history
    return_dict['training_time'] = training_time
    # return_dict['y_test'] = y_test
    # return_dict['y_pred'] = y_pred
    models_rnn.append(model)
    keras.backend.clear_session()  # Free GPU VRAM

# Function to create and train model for a given set of hyperparameters
# def create_and_train_model_LSTM(batch_size, learning_rate, x_train, y_train, x_val, y_val):
def evaluate_model_lstm(params, training_generator: DataGenerator, validation_generator: DataGenerator):
    model = Sequential()
    model.add(InputLayer(training_generator.getDim()))
    model.add(LSTM(256, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.1))
    model.add(LSTM(64))
    model.add(Dropout(0.1))
    model.add(Dense(2, activation="softmax"))
    model.compile(loss="sparse_categorical_crossentropy", optimizer=Adam(learning_rate=params['learning_rate']), metrics=["sparse_categorical_accuracy", precision_m, recall_m, f1_m])
    
    callbacks = [
        keras.callbacks.ModelCheckpoint(
            BEST_LSTM_CPT, save_best_only=True, monitor="val_loss"
        ),
        keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=params['patience'], restore_best_weights=True
        ),
    ]
    
    start_time = time.time()
    history = model.fit(
        training_generator,
        epochs=params['epochs'], 
        batch_size=params['batch_size'],
        callbacks=callbacks,
        validation_data=validation_generator,
        verbose=2,
        use_multiprocessing=True,
        workers=10
    )
    end_time = time.time()
    
    training_time = end_time - start_time

    _, accuracy, precision, recall, f1 = model.evaluate(validation_generator, verbose=2)
    # y_pred = model.predict(x_test, verbose=2)
    # y_pred_classes = np.argmax(y_pred, axis=-1)

    # accuracy = accuracy_score(y_test, y_pred_classes)
    # precision = precision_score(y_test, y_pred_classes)
    # recall = recall_score(y_test, y_pred_classes)
    # f1 = f1_score(y_test, y_pred_classes)

    return accuracy, precision, recall, f1, history.history, model, training_time

def run_evaluation_lstm(params, training_generator, validation_generator, return_dict, models_rnn):
    accuracy, precision, recall, f1, history, model, training_time = evaluate_model_lstm(params, training_generator, validation_generator)
    return_dict['accuracy'] = accuracy
    return_dict['precision'] = precision
    return_dict['recall'] = recall
    return_dict['f1'] = f1
    return_dict['history'] = history
    return_dict['training_time'] = training_time
    # return_dict['y_test'] = y_test
    # return_dict['y_pred'] = y_pred
    models_rnn.append(model)
    keras.backend.clear_session()  # Free GPU VRAM


TRANSFORMER_MODEL = {
    'num_transformer_blocks': 4,
    'head_size': 128,
    'num_heads': 6,
    'ff_dim': 512,
    'learning_rate': 0.001,
    'mlp_units': [128],
    'dropout': 0.1,
    'batch_size': 96,
    'epochs': EPOCHS,
    'patience': PATIENCE
}

# TODO: Find the optimum of the following
LSTM_MODEL = {
    'batch_size': 16,
    'learning_rate': 1e-05,
    'epochs': EPOCHS,
    'patience': PATIENCE
}

MODELS = {
    'TRANSFORMER': TRANSFORMER_MODEL,
    'LSTM': LSTM_MODEL
}

os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# Function to save checkpoints
def save_checkpoint(filename, data):
    with open(os.path.join(CHECKPOINT_DIR, filename), 'wb') as f:
        pickle.dump(data, f)

# Function to load checkpoints
def load_checkpoint(filename):
    if os.path.exists(os.path.join(CHECKPOINT_DIR, filename)):
        with open(os.path.join(CHECKPOINT_DIR, filename), 'rb') as f:
            return pickle.load(f)
    return None

# Perform K-Fold Cross-Validation
kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)

# Collect metrics for both models
model_metrics = {key: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'training_time': []} for key in MODELS.keys()}
histories = {key: [] for key in MODELS.keys()}

# Load saved state if available
saved_state = load_checkpoint(FILE_NAME)
current_fold = 0
if saved_state:
    model_metrics, histories, current_fold = saved_state
else:
    current_fold = 0


manager = Manager()
return_dict = manager.dict()
manager2 = Manager()
models_rnn = manager2.list()

datagen_params = {'dim': (227, 34),
          'batch_size': 0,
          'n_channels': 1,
          'shuffle': True,
          'data_folder': DATA_FOLDER
}

''' 
    train_index -> list of training samples ids (length = 253917 90% 10-fold) 
    val_index -> list of validation samples ids (length = 28213 10% 10-fold) 
'''
for fold_index, (train_index, val_index) in enumerate(kf.split(range(NUM_SAMPLES))):
    if fold_index < current_fold:
        print('Checkpoint found, skipping fold:', fold_index)
        continue  # Skip already processed folds

    # TODO: Modify to fit the generator
    # x_fold_train, x_fold_val = x_data[train_index], x_data[val_index]
    # y_fold_train, y_fold_val = y_data[train_index], y_data[val_index]

    # partition = {'train': train_index, 'validation': val_index}

    for key, params in MODELS.items():
        keras.backend.clear_session()
        print(f'Training {key} in FOLD: {fold_index}, with parameters:', params)
        time.sleep(5)

        datagen_params['batch_size'] = MODELS[key]['batch_size']
        training_generator = DataGenerator(train_index, **datagen_params)
        validation_generator = DataGenerator(val_index, **datagen_params)

        if key == 'TRANSFORMER':
            p = Process(target=run_evaluation_transformer, args=(params, training_generator, validation_generator, return_dict, models_rnn))
        elif key == 'LSTM':
            p = Process(target=run_evaluation_lstm, args=(params, training_generator, validation_generator, return_dict, models_rnn))
        
        p.start()
        p.join()  # Wait for the subprocess to finish
        # p.kill()

        accuracy = return_dict['accuracy']
        precision = return_dict['precision']
        recall = return_dict['recall']
        f1 = return_dict['f1']
        history = return_dict['history']
        training_time = return_dict['training_time']
        # y_test = return_dict['y_test']
        # y_pred = return_dict['y_pred']

        
        model_metrics[key]['accuracy'].append(accuracy)
        model_metrics[key]['precision'].append(precision)
        model_metrics[key]['recall'].append(recall)
        model_metrics[key]['f1'].append(f1)
        model_metrics[key]['training_time'].append(training_time)
        # model_metrics[key]['y_test'].append(y_test)
        # model_metrics[key]['y_pred'].append(y_pred)
        histories[key].append(history)

    # Save state after each fold
    current_fold += 1
    save_checkpoint(FILE_NAME, (model_metrics, histories, current_fold))

# Perform t-tests to compare models
model_keys = list(MODELS.keys())
for i in range(len(model_keys)):
    for j in range(i + 1, len(model_keys)):
        key1 = model_keys[i]
        key2 = model_keys[j]

        print(f'\nComparing {key1} vs {key2}:')

        accuracy_ttest = ttest_rel(model_metrics[key1]['accuracy'], model_metrics[key2]['accuracy'])
        precision_ttest = ttest_rel(model_metrics[key1]['precision'], model_metrics[key2]['precision'])
        recall_ttest = ttest_rel(model_metrics[key1]['recall'], model_metrics[key2]['recall'])
        f1_ttest = ttest_rel(model_metrics[key1]['f1'], model_metrics[key2]['f1'])
        training_time_ttest = ttest_rel(model_metrics[key1]['training_time'], model_metrics[key2]['training_time'])

        accuracy_wilcoxon = wilcoxon(model_metrics[key1]['accuracy'], model_metrics[key2]['accuracy'])
        precision_wilcoxon = wilcoxon(model_metrics[key1]['precision'], model_metrics[key2]['precision'])
        recall_wilcoxon = wilcoxon(model_metrics[key1]['recall'], model_metrics[key2]['recall'])
        f1_wilcoxon = wilcoxon(model_metrics[key1]['f1'], model_metrics[key2]['f1'])
        training_time_wilcoxon = wilcoxon(model_metrics[key1]['training_time'], model_metrics[key2]['training_time'])

        print(f"Accuracy t-test: p-value={accuracy_ttest.pvalue}")
        print(f"Precision t-test: p-value={precision_ttest.pvalue}")
        print(f"Recall t-test: p-value={recall_ttest.pvalue}")
        print(f"F1-score t-test: p-value={f1_ttest.pvalue}")
        print(f"Training time t-test: p-value={training_time_ttest.pvalue}")

        print(f"\nAccuracy Wilcoxon test: p-value={accuracy_wilcoxon.pvalue}")
        print(f"Precision Wilcoxon test: p-value={precision_wilcoxon.pvalue}")
        print(f"Recall Wilcoxon test: p-value={recall_wilcoxon.pvalue}")
        print(f"F1-score Wilcoxon test: p-value={f1_wilcoxon.pvalue}")
        print(f"Training time Wilcoxon test: p-value={training_time_wilcoxon.pvalue}")

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


## TODO: Later rerun the experiment and from the fold indices (random_state = 42) find the y_test(y_val actually) and print the confusion matrices
# Plot confusion matrix and store locally
# print('\n')
# os.makedirs(f'{IMAGE_DIR}/confusion_matrices', exist_ok=True)

# # Initialize lists to collect confusion matrices
# transformer_cms = []
# lstm_cms = []

# for key in MODELS.keys():
#     for i in range(len(model_metrics[key]['y_pred'])):
#         cm = confusion_matrix(model_metrics[key]['y_test'][i], model_metrics[key]['y_pred'][i])
#         # print(f'{key} {i} Confusion Matrix')
#         # print(cm)

#         # Plot confusion matrix using ConfusionMatrixDisplay
#         disp = ConfusionMatrixDisplay(confusion_matrix=cm)
#         disp.plot(cmap=plt.cm.Blues)

#         plt.title(f'{key} {i} Confusion Matrix')
#         plt.savefig(f'{IMAGE_DIR}/confusion_matrices/{key.lower()}_{i}.jpg')
#         plt.show()
#         plt.close()

#         # Collect confusion matrices
#         if 'TRANSFORMER' in key.upper():
#             transformer_cms.append(cm)
#         elif 'LSTM' in key.upper():
#             lstm_cms.append(cm)

# # Function to aggregate confusion matrices
# def aggregate_confusion_matrices(cms):
#     aggregate_cm = np.sum(cms, axis=0)
#     return aggregate_cm

# # Aggregating the confusion matrices
# if transformer_cms:
#     transformer_aggregate_cm = aggregate_confusion_matrices(transformer_cms)
#     # Plotting and saving the aggregated confusion matrix
#     disp = ConfusionMatrixDisplay(confusion_matrix=transformer_aggregate_cm)
#     disp.plot(cmap=plt.cm.Blues)
#     plt.title('Aggregated Transformer Confusion Matrix')
#     plt.savefig(f'{IMAGE_DIR}/confusion_matrices/transformer_aggregate.jpg')
#     plt.show()
#     plt.close()

# if lstm_cms:
#     lstm_aggregate_cm = aggregate_confusion_matrices(lstm_cms)
#     # Plotting and saving the aggregated confusion matrix
#     disp = ConfusionMatrixDisplay(confusion_matrix=lstm_aggregate_cm)
#     disp.plot(cmap=plt.cm.Blues)
#     plt.title('Aggregated LSTM Confusion Matrix')
#     plt.savefig(f'{IMAGE_DIR}/confusion_matrices/lstm_aggregate.jpg')
#     plt.show()
#     plt.close()


for key, fold_histories in histories.items():
    metric = "sparse_categorical_accuracy"

    BATCH_SIZE = MODELS[key]['batch_size']
    LEARNING_RATE = MODELS[key]['learning_rate']
    PARAMS = MODELS[key].copy()
    PARAMS.pop('batch_size', None)
    PARAMS.pop('learning_rate', None)
    
    for fold_index, history in enumerate(fold_histories):
        plt.figure()
        plt.plot(history['sparse_categorical_accuracy'])
        plt.plot(history["val_sparse_categorical_accuracy"])
        plt.title(f"Sparse Categorical Accuracy {key.lower()} - Fold {fold_index+1}")
        plt.ylabel("Sparse Categorical Accuracy", fontsize="large")
        plt.xlabel("Epoch", fontsize="large")
        plt.legend(["Train", "Validation"], loc="best")
        plt.savefig(f'{IMAGE_DIR}/training_{key.lower()}_accuracy_fold{fold_index+1}_epochs{EPOCHS}_BS{BATCH_SIZE}_LR{LEARNING_RATE}.jpg')
        plt.show()
        plt.close()

    metric = "loss"

    for fold_index, history in enumerate(fold_histories):
        plt.figure()
        plt.plot(history['loss'])
        plt.plot(history["val_loss"])
        plt.title(f"Training and Validation Loss {key.lower()} - Fold {fold_index+1}")
        plt.ylabel("Loss", fontsize="large")
        plt.xlabel("Epoch", fontsize="large")
        plt.legend(["Train", "Validation"], loc="best")
        plt.savefig(f'{IMAGE_DIR}/training_{key.lower()}_loss_fold{fold_index+1}_epochs{EPOCHS}_BS{BATCH_SIZE}_LR{LEARNING_RATE}.jpg')
        plt.show()
        plt.close()


# Calculate and print average metrics and training time
from statistics import mean

for model_name, metrics in model_metrics.items():

    print(f"\n{model_name} metrics:")
    print('Accuracy:', model_metrics[model_name]['accuracy'])
    print('Precision:', model_metrics[model_name]['precision'])
    print('Recall:', model_metrics[model_name]['recall'])
    print('F1-score:', model_metrics[model_name]['f1'])
    print('Training Time (s):', metrics['training_time'])

import statistics

for model_name, metrics in model_metrics.items():
    print(f"\n{model_name} average metrics:")

    accuracy_mean = statistics.mean(metrics['accuracy'])
    accuracy_median = statistics.median(metrics['accuracy'])
    accuracy_std = statistics.stdev(metrics['accuracy'])
    print('Accuracy: Mean =', accuracy_mean, 'Median =', accuracy_median, 'Std =', accuracy_std)

    precision_mean = statistics.mean(metrics['precision'])
    precision_median = statistics.median(metrics['precision'])
    precision_std = statistics.stdev(metrics['precision'])
    print('Precision: Mean =', precision_mean, 'Median =', precision_median, 'Std =', precision_std)

    recall_mean = statistics.mean(metrics['recall'])
    recall_median = statistics.median(metrics['recall'])
    recall_std = statistics.stdev(metrics['recall'])
    print('Recall: Mean =', recall_mean, 'Median =', recall_median, 'Std =', recall_std)

    f1_mean = statistics.mean(metrics['f1'])
    f1_median = statistics.median(metrics['f1'])
    f1_std = statistics.stdev(metrics['f1'])
    print('F1-score: Mean =', f1_mean, 'Median =', f1_median, 'Std =', f1_std)

    training_time_mean = statistics.mean(metrics['training_time'])
    training_time_median = statistics.median(metrics['training_time'])
    training_time_std = statistics.stdev(metrics['training_time'])
    print('Average Training Time (s): Mean =', training_time_mean, 'Median =', training_time_median, 'Std =', training_time_std)



def find_first_less_than(arr, threshold):
    return next((i for i, value in enumerate(arr) if value < threshold), EPOCHS)

def find_first_greater_than(arr, threshold):
    return next((i for i, value in enumerate(arr) if value > threshold), EPOCHS)

for model in histories.keys():
    val_index_loss = 0
    val_index_acc = 0
    for fold in histories[model]:
        temp = find_first_less_than(fold['val_loss'], 0.1)
        val_index_loss += temp
        temp = find_first_greater_than(fold['val_sparse_categorical_accuracy'], 0.95)
        val_index_acc += temp

    print(f'\nAverage Index for {model} where val_loss < 0.1:', val_index_loss / FOLDS)
    print(f'Average Index for {model} where val_acc > 0.95:', val_index_acc / FOLDS)

